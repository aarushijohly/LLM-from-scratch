{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fae36585-1e26-4967-9e29-2d9ed29c22b4",
   "metadata": {},
   "source": [
    "# Creating token embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c19bcdcd-a899-4ed2-bb52-45218cee9f3d",
   "metadata": {},
   "source": [
    "Illustration of how the token ID to embedding vector conversion works with a hands on example.\n",
    "\n",
    "Suppose we have the following four tokens with IDs 2, 3, 5 and 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "6df6c13a-833a-4e5b-9079-beb304806d92",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import tiktoken"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "90bfa316-4b6b-41cd-ae24-3a23b66fefb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"the-verdict.txt\", \"r\", encoding=\"utf-8\") as f:\n",
    "    raw_text = f.read()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49075137-a0c5-4b3f-a789-43c381c51c53",
   "metadata": {},
   "source": [
    "For the sake of simplicity we are going to use a small vocabulary of 6 words(instead of 50,257 words in the BPE tokenizer vocabulary), and we want to create embeddings of size 3(in GPT-3 the embedding size is 12,288 dimentions).\n",
    "\n",
    "\n",
    "vocab: Each of these 6 words will be mapped in R3\n",
    "\n",
    "quick-->4\n",
    "\n",
    "fox-->0\n",
    "\n",
    "is-->3\n",
    "\n",
    "in-->2\n",
    "\n",
    "the-->5\n",
    "\n",
    "house-->1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ea0a4a8-38a8-43de-b660-edcc13df41f1",
   "metadata": {},
   "source": [
    "torch.nn.Embedding return a simple lookup table that stores embeddings of a fixed dictionary and size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "7cac87a2-ea10-49e4-90e5-53314b8d2c85",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size=6\n",
    "output_dim=3\n",
    "input_ids = torch.tensor([2,3,5,1])\n",
    "\n",
    "torch.manual_seed(123)\n",
    "embedding_layer = torch.nn.Embedding(vocab_size, output_dim) #creates a dictionary\n",
    "#this initialize the weights of the embedding matrix in a random manner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "aae3722f-bc67-471b-8aa1-a31d2b8830ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameter containing:\n",
      "tensor([[ 0.3374, -0.1778, -0.1690],\n",
      "        [ 0.9178,  1.5810,  1.3010],\n",
      "        [ 1.2753, -0.2010, -0.1606],\n",
      "        [-0.4015,  0.9666, -1.1481],\n",
      "        [-1.1589,  0.3255, -0.6315],\n",
      "        [-2.8400, -0.7849, -1.4096]], requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "print(embedding_layer.weight)\n",
    "#these are the initial weights which needs to be optimized during LLM training as part of the LLM optmization itself."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "34b73477-c09c-475a-88ee-ec9e403b8874",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-0.4015,  0.9666, -1.1481]], grad_fn=<EmbeddingBackward0>)\n"
     ]
    }
   ],
   "source": [
    "print(embedding_layer(torch.tensor([3])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "098b5b95-c7d6-458b-afff-a20babca5ad7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 1.2753, -0.2010, -0.1606],\n",
      "        [-0.4015,  0.9666, -1.1481],\n",
      "        [-2.8400, -0.7849, -1.4096],\n",
      "        [ 0.9178,  1.5810,  1.3010]], grad_fn=<EmbeddingBackward0>)\n"
     ]
    }
   ],
   "source": [
    "print(embedding_layer(input_ids))\n",
    "\n",
    "# a look up operation that retrieves rows from the embedding layer weight matrix using a token ID."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2665d81-fb92-4800-8c96-380afb0f1fdb",
   "metadata": {},
   "source": [
    "## Positional embeddings (Encoding word positions):\n",
    "\n",
    "We encode the input tokens into 256-dimensional vector representation.\n",
    "\n",
    "Assumption: The token IDs were created by the BPE tokenizer which has a vocabulary size of 50,257."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "caf83ef4-ebc6-4e60-8927-39856c9f45ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset, DataLoader\n",
    "class GPTDatasetV1(Dataset):\n",
    "    def __init__(self, txt, tokenizer, max_length, stride): #max length = context size\n",
    "        self.input_ids = []\n",
    "        self.target_ids = []\n",
    "        #tokenize the entire text\n",
    "        token_ids = tokenizer.encode(txt, allowed_special={\"<|endoftxt|>\"})\n",
    "        #using a sliding window to chunk the book into overlapping sequences of max_length\n",
    "        for i in range(0, len(token_ids)-max_length, stride):\n",
    "            input_chunk = token_ids[i:i+max_length]\n",
    "            target_chunk = token_ids[i+1: i+max_length+1]\n",
    "            self.input_ids.append(torch.tensor(input_chunk))\n",
    "            self.target_ids.append(torch.tensor(target_chunk))\n",
    "            \n",
    "    def __len__(self):\n",
    "        return len(self.input_ids)\n",
    "        \n",
    "    #method which will be used by the data loader\n",
    "    def __getitem__(self, idx): #idx=index\n",
    "        return self.input_ids[idx], self.target_ids[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "e9e7d6b2-4110-4a9d-a36d-b622e9ed1160",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_dataloader_v1(txt, batch_size=4, max_length=256, stride=128, shuffle=True, drop_last=True, num_workers=0):\n",
    "    #initialize the tokenizer\n",
    "    tokenizer = tiktoken.get_encoding(\"gpt2\")\n",
    "    #create dataset\n",
    "    dataset = GPTDatasetV1(txt, tokenizer, max_length, stride)\n",
    "    #create dataloader, this function will check the get item method in above function and it will return the input output pairs based on what\n",
    "    #is mentioned in the get item.\n",
    "    dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=shuffle, drop_last=drop_last, num_workers=num_workers)\n",
    "    return dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "6740bb24-1701-4755-b582-b1c62fde9d6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size=50257\n",
    "optput_dim=256 #GPT-3 has the embedding size of 12,288\n",
    "token_embedding_layers = torch.nn.Embedding(vocab_size, output_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "92c94b95-65f4-41ec-8956-93ab1b14bc81",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_length=4\n",
    "dataloader=create_dataloader_v1(raw_text,batch_size=8,max_length=max_length,stride=max_length,shuffle=False)\n",
    "data_iter=iter(dataloader)\n",
    "inputs, targets=next(data_iter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "5fe09bc7-f3b8-4731-a848-acd0fe608610",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token IDs:\n",
      " tensor([[   40,   367,  2885,  1464],\n",
      "        [ 1807,  3619,   402,   271],\n",
      "        [10899,  2138,   257,  7026],\n",
      "        [15632,   438,  2016,   257],\n",
      "        [  922,  5891,  1576,   438],\n",
      "        [  568,   340,   373,   645],\n",
      "        [ 1049,  5975,   284,   502],\n",
      "        [  284,  3285,   326,    11]])\n",
      "\n",
      "Inputs shape:\n",
      " torch.Size([8, 4])\n"
     ]
    }
   ],
   "source": [
    "print(\"Token IDs:\\n\", inputs)\n",
    "print(\"\\nInputs shape:\\n\", inputs.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "fc77b307-791c-4058-9582-efc040284b05",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([8, 4, 3])\n"
     ]
    }
   ],
   "source": [
    "token_embeddings=token_embedding_layers(inputs)\n",
    "print(token_embeddings.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4889cea2-431d-4f45-8349-555c13dfd94b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbf3b07c-9b97-4f82-9bd3-085aa103d570",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec0f5c35-2eb9-4996-9495-a9a723aa5635",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "068e993a-8b32-4c31-b985-ec0b49e9c5bf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69ce1ac8-e9fc-4d16-8618-45f214617660",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc1e25de-701f-474c-b0bb-b2d17be52a88",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70a69e76-8a92-49db-b7bb-1dc3f9840bee",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2995f5c8-8d5b-4f9a-aaeb-fa0afe83302d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2387d79-e1d1-458f-bb93-9d6d9dab7c97",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ae19fc5-e1bf-4de3-b440-7e7c0a0c3bea",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3ca10cf-939b-4c1b-890d-6a6ba8047b68",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b6a56b4-86ef-4fd0-8cee-8b9322c15657",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a15588b9-9c36-47a9-b930-2a8b3b3b84de",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a3377c5-2759-4b19-bb0e-a2dfe1fc1b12",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad56c072-8ec8-42d0-a522-9a83282473a5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
