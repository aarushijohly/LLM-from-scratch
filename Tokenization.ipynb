{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6b8869b0-8e6f-4db6-b1dd-d9e05d51a75d",
   "metadata": {},
   "source": [
    "## Implemented two simple tokenizers from scratch and demonstrated tiktoken library.\n",
    "Dataset used: 'The verdict' by Edith Warton(1908)\n",
    "Embedding: The process of converting data into a vector format.\n",
    "Implementing the first step of data prepration and sampling: Tokenization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6326149f-6b29-48d4-b131-7e702c4b5d9e",
   "metadata": {},
   "source": [
    "Step 1: Creating tokens (word based tokenizers) or tokenizing text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54c86a97-48d3-4b07-a7e5-4418a0895898",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"the-verdict.txt\", \"r\", encoding=\"utf-8\") as f:\n",
    "    raw_text = f.read()"
   ]
  },
  {
   "cell_type": "raw",
   "id": "fe541346-0263-4ea8-b0a6-7be8c6446b47",
   "metadata": {},
   "source": [
    "import re\n",
    "preprocessed = re.split(r'([,.:;?_!\"()\\']| |--|\\\\s)', raw_text)\n",
    "preprocessed = [item.strip() for item in preprocessed if item.strip()]\n",
    "print(preprocessed[:10])\n",
    "\n",
    "all_words = sorted(set(preprocessed))\n",
    "vocab_size = len(all_words)\n",
    "vocab = {token:integer for integer,token in enumerate(all_words)}\n",
    "print(f\"Vocabulary size: {vocab_size}\")"
   ]
  },
  {
   "cell_type": "raw",
   "id": "bab2eb06-917b-4155-a823-518204a60830",
   "metadata": {},
   "source": [
    "class SimpleTokenizerV1:\n",
    "    def __init__(self, vocab):\n",
    "        self.str_to_int = vocab\n",
    "        self.int_to_str = {t:s for s, t in vocab.items()}\n",
    "\n",
    "    def encode(self, text):\n",
    "        preprocessed = re.split(r'[,.:;?_!\"()\\']|--|\\s', text)\n",
    "        preprocessed = [item.strip() for item in preprocessed if item.strip()]\n",
    "        ids = [self.str_to_int[s] for s in preprocessed]\n",
    "        return ids\n",
    "\n",
    "    def decode(self, ids):\n",
    "        text = \" \".join([self.int_to_str[i] for i in ids])\n",
    "        text = re.sub(r'\\s+([,.?!\"()\\'])', r'\\1', text)\n",
    "        return text"
   ]
  },
  {
   "cell_type": "raw",
   "id": "b41b727b-b36e-4e07-b146-994963c47c27",
   "metadata": {},
   "source": [
    "tokenizer = SimpleTokenizerV1(vocab)\n",
    "text = \"\"\"\"It's the last he painted, you know,\"\n",
    "        Mrs. Gisburn said with pardonable pride.\"\"\"\n",
    "ids = tokenizer.encode(text)\n",
    "print(ids)\n",
    "\n",
    "tokenizer.decode(ids)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "3cd76184-275a-4606-aedf-86addbf6bf76",
   "metadata": {},
   "source": [
    "text = \"Hello, do you like tea?\"\n",
    "#print(tokenizer.encode(text))"
   ]
  },
  {
   "cell_type": "raw",
   "id": "110443ab-31cf-4b45-8642-5f82ac81925e",
   "metadata": {},
   "source": [
    "all_tokens = sorted(set(preprocessed))\n",
    "all_tokens.extend([\"<|endoftext|>\", \"<|unk|>\"])\n",
    "vocab = {token:integer for integer,token in enumerate(all_tokens)}\n",
    "len(vocab)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "4591a0ff-3478-4b06-9d75-11b84945c145",
   "metadata": {},
   "source": [
    "class SimpleTokenizerV2:\n",
    "    def __init__(self, vocab):\n",
    "        self.toInt = vocab\n",
    "        self.toStr = {integer:token for token, integer in vocab.items()}\n",
    "\n",
    "    def encode(self, text):\n",
    "        preprocessed = re.split(r'([,.:;?_!()\\']|--|\\s)', text)\n",
    "        preprocessed = [item.strip() for item in preprocessed if item.strip()]\n",
    "        preprocessed = [item if item in self.toInt else \"<|unk|>\" for item in preprocessed]\n",
    "        ids = [self.toInt[s] for s in preprocessed]\n",
    "        return ids\n",
    "\n",
    "    def decode(self, ids):\n",
    "        text = \" \".join([self.toStr[i] for i in ids])\n",
    "        text = re.sub(r'\\s+([,.?!\"()\\'])', r'\\1', text)\n",
    "        return text"
   ]
  },
  {
   "cell_type": "raw",
   "id": "f12167c3-d111-4bed-be9f-2a5fb539d435",
   "metadata": {},
   "source": [
    "tokenizer = SimpleTokenizerV2(vocab)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "85359768-f339-4c1b-8e54-1c10e4b1956a",
   "metadata": {},
   "source": [
    "text1 = \"Hello, do you like tea?\"\n",
    "text2 = \"In the sunlit terraces of the palace.\"\n",
    "text = \" <|endoftext|> \".join((text1, text2))\n",
    "print('text: ', text)\n",
    "ids = tokenizer.encode(text)\n",
    "print(ids)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "04571cef-a74b-499d-b3ff-93e85052b635",
   "metadata": {},
   "source": [
    "tokenizer.decode(ids)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "117be90b-929b-4382-851f-d3626a5dd918",
   "metadata": {},
   "source": [
    "## BYTE PAIR ENCODING\n",
    "Implementing BPE from scratch can be relatively complicated, thus we will use an existing Python open-source library called tiktoken which is a fast BPE tokenizer for use wuth OPENAI's models.\n",
    "\n",
    "Encode: Tokenization and conversion into token IDs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afac4977-0770-4205-a8ec-404097571573",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import tiktoken\n",
    "import importlib\n",
    "print('tiktoken version: ', importlib.metadata.version(\"tiktoken\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5775dcc9-1ac5-4df1-afac-6884d050fe18",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = tiktoken.get_encoding(\"gpt2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "654b70dd-8292-4951-ae05-bafe36313bf8",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = ( \"Hello, do you like tea? <|endoftext|> In the sunlit terraces of someunknownPlace.\" )\n",
    "\n",
    "integers = tokenizer.encode(text, allowed_special={\"<|endoftext|>\"})\n",
    "print(integers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d80a41c-654c-4bab-9674-d743653faf38",
   "metadata": {},
   "outputs": [],
   "source": [
    "strings = tokenizer.decode(integers)\n",
    "print(strings)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b1850a7-5825-4365-9c29-77af4fda8dd2",
   "metadata": {},
   "source": [
    "The BPE tokenizer can handle unknown words. How can it achieve this without using <|unk|> token?\n",
    "\n",
    "\n",
    "The algorithm underlying BPE breaksdown words that aren't in its predefined vocabulary into smaller subword units or even individual characters.\n",
    "This enables it to handle out of vocabulary(OOV) words."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63e82bf2-56ea-4a75-8536-92edfab8027d",
   "metadata": {},
   "source": [
    "An example to illustrate how the BPE tokenizer deals with unknown tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58497384-078d-481d-b712-96fe37070a1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "integers = tokenizer.encode(\"Akwirw ier\")\n",
    "print(f\"integers: {integers}\")\n",
    "\n",
    "strings = tokenizer.decode(integers)\n",
    "print(f\"strings: {strings}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf9d1f54-33d8-4ac4-8603-5656d5a1a04e",
   "metadata": {},
   "source": [
    "## Creating input-output layer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2ef66ff-2143-4cfb-bfe9-4e34f61c5a19",
   "metadata": {},
   "source": [
    "We implement a data loader that fetches the input-output pairs using a sliding window approach."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d81f116-5756-45b1-aaf1-21781a91725c",
   "metadata": {},
   "outputs": [],
   "source": [
    "enc_text = tokenizer.encode(raw_text)\n",
    "print(f\"Length of enc_text: {len(enc_text)}\")\n",
    "enc_sample = enc_text[50:]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9586493-8563-4dc5-83dd-f20e7c7c1bdc",
   "metadata": {},
   "source": [
    "## Context size \n",
    "Determines how many tokens are included in the input. The model istrained to look at a sequence of context_size number of words to predict the next word in the sequence.\n",
    "\n",
    "Each input-output pair contains context size number of prediction tasks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6aa5f2d8-99b9-4ab1-ba12-9406a324c852",
   "metadata": {},
   "outputs": [],
   "source": [
    "context_size = 4\n",
    "x = enc_sample[:context_size]\n",
    "y = enc_sample[1:context_size+1]\n",
    "\n",
    "print(f\"x:  {x}\")\n",
    "print(f\"y:    {y}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb71113f-70a8-4d2f-8f5e-1bb3fe73794f",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(1, context_size+1):\n",
    "    context = enc_sample[:i]\n",
    "    desired = enc_sample[i]\n",
    "    print(context, \"---->\", desired)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38d3e8ff-85b8-4633-ad52-83345ab9949f",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(1, context_size+1):\n",
    "    context = enc_sample[:i]\n",
    "    desired = enc_sample[i]\n",
    "    print(tokenizer.decode(context), \"---->\", tokenizer.decode([desired]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d47d118b-5301-4608-8bcd-5fb62d53d0f3",
   "metadata": {},
   "source": [
    "## Data Loader\n",
    "Iterates over the input dataset and returns inputs and targets as PyTorch tensors. We are iterested in returning two tensors: an input tensor containing text that the LLM sees and a target tensor that includes the trget for LLM to predict. We implement dataloader using PyTorch datasets and dataloader classes. We aim at returning two tensors: an input tensor and an output tensor. Helps us do parallel processing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7425667-1f25-40ef-9f3d-2ce2faf7d021",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "class GPTDatasetV1(Dataset):\n",
    "    def __init__(self, txt, tokenizer, max_length, stride):\n",
    "        self.input_ids = []\n",
    "        self.target_ids = []\n",
    "        self.token_ids = tokenizer.encode(txt, allowed_special={\"<|endoftext|>\"})\n",
    "\n",
    "        for i in range(0, len(self.token_ids)-max_length, stride):\n",
    "            input_chunk = self.token_ids[i : max_length+i]\n",
    "            target_chunk = self.token_ids[i+1 : i + max_length + 1]\n",
    "            self.input_ids.append(torch.tensor(input_chunk))\n",
    "            self.target_ids.append(torch.tensor(target_chunk))\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.input_ids)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.input_ids[idx], self.target_ids[idx] #idx=index"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0296a620-cdf1-449f-ac73-29e49c71c629",
   "metadata": {},
   "source": [
    "drop_last=true: drops the last batch if it is shorter than the specified batch_size to prevent loss spikes during training.\n",
    "\n",
    "batch_size = how many batches or CPU processes we want to run parallelly\n",
    "\n",
    "max_length = context length\n",
    "\n",
    "num_workers = number of CPU threads which we can run simultaneously. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "212c711e-807d-446a-a6f2-05051cb15ed5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_dataloader_v1(txt, batch_size=4, max_length=256, stride=128, shuffle=True, drop_last=True, num_workers=0):\n",
    "    tokenizer = tiktoken.get_encoding(\"o200k_base\")\n",
    "    dataset = GPTDatasetV1(txt, tokenizer, max_length, stride) #creating dataset\n",
    "    dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=shuffle, drop_last=drop_last, num_workers=num_workers)\n",
    "    return dataloader\n",
    "\n",
    "#this code uses the GPTDatasetV1 to load the inputs in batches via a PyTorch DataLoader.\n",
    "# this function governs batch processing or the parallel processing we need which is governed by the batch size.\n",
    "#It help us create the input output data pairs from the dataset which we defined earlier.   "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8dcdec02-b511-4df8-a1da-1e0e6eeaa1e9",
   "metadata": {},
   "source": [
    "We now convert the dataloader to python iterator to fetch the next entry via python built-in next() function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be00c565-6230-4476-bc78-f1954761b76a",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"PyTorch Version: \", torch.__version__)\n",
    "dataloader = create_dataloader_v1(raw_text, batch_size=8, max_length=4, stride=4, shuffle=False)\n",
    "data_iter = iter(dataloader)\n",
    "inputs, targets = next(data_iter)\n",
    "print(\"Inputs: \", inputs)\n",
    "print(\"targets: \", targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f71efdd0-9b2b-4a75-b1f8-b7e81dc5b22f",
   "metadata": {},
   "outputs": [],
   "source": [
    "second_batch = next(data_iter)\n",
    "print(second_batch)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f747d6cf-d312-4844-a4a0-e3d392f77570",
   "metadata": {},
   "source": [
    "Small batch sizes require less memort during training but lead to more noisy model updates. Batch size is a trade-off and hyperparameter to experiment with when training LLMs. Model will procces one batch before making the parameter updates."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c8b632d-2d15-4e1f-9574-e5b0b33e7e44",
   "metadata": {},
   "source": [
    "## TOKEN EMBEDDINGS"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f9f760d-a5bf-4bb1-8ead-f16a0869ceb1",
   "metadata": {},
   "source": [
    "For demonstration purpose we create an embeding later for a vocab of cardinality 6 projected to R3."
   ]
  },
  {
   "cell_type": "raw",
   "id": "2bee72ab-52a9-4798-9277-a787ea865040",
   "metadata": {},
   "source": [
    "vocab_size = 6\n",
    "output_dim = 3\n",
    "\n",
    "torch.manual_seed(123)\n",
    "embedding_layer = torch.nn.Embedding(vocab_size, output_dim)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "7045b650-a1ee-4b29-98e4-84a9fa10e11c",
   "metadata": {},
   "source": [
    "print(\"weights:\\n\", embedding_layer.weight)\n",
    "print(\"third vector:\\n\", embedding_layer(torch.tensor([3])))"
   ]
  },
  {
   "cell_type": "raw",
   "id": "9c6f0aad-8e63-42df-91cd-b27001f8925f",
   "metadata": {},
   "source": [
    "input_ids = torch.tensor([2,3,5,1])"
   ]
  },
  {
   "cell_type": "raw",
   "id": "96cc0031-a62c-43d3-b2b7-d4035132cce5",
   "metadata": {},
   "source": [
    "print(embedding_layer(input_ids))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c920a57-1d8c-4962-9f4a-434740afd9fd",
   "metadata": {},
   "source": [
    "## Positional encoding/embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57affa97-ab0c-489e-b842-3041ff29b89a",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size = 50257\n",
    "output_dim = 256\n",
    "\n",
    "token_embedding_layer = torch.nn.Embedding(vocab_size, output_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c077286-8990-4147-b786-ac68660cbafc",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_length = 4\n",
    "dataloader = create_dataloader_v1(raw_text, batch_size=8, max_length=max_length, stride=max_length, shuffle=False)\n",
    "\n",
    "data_iter = iter(dataloader)\n",
    "inputs, targets = next(data_iter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca9e6e5b-6f19-4181-8cc7-e5ac006e4aee",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Token IDs:\\n\", inputs)\n",
    "print(\"\\nInputs shape:\\n\", inputs.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "411cef3b-edbb-41f0-9346-24181a170aff",
   "metadata": {},
   "source": [
    "Now we convert each token id to a 256 dimensional vector."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1177d182-eaf6-43af-8fb0-f8d7cb3386db",
   "metadata": {},
   "outputs": [],
   "source": [
    "token_embeddings = token_embedding_layer(inputs)\n",
    "print(token_embeddings.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17632911-0818-4797-a475-2dc314a44dfd",
   "metadata": {},
   "source": [
    "Embedding layer for positional embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d0bd2e7-27be-4d0f-b02b-6091520a93e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "context_length = max_length\n",
    "pos_embedding_layer = torch.nn.Embedding(context_length, output_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9198a1c4-3b04-478d-95e0-2772c38b8a91",
   "metadata": {},
   "outputs": [],
   "source": [
    "pos_embeddings = pos_embedding_layer(torch.arange(max_length))\n",
    "print(pos_embeddings.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b93848d-4047-4bb4-ba81-f42d44f162f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "token_embedding = input_embeddings + positonal_embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb943f43-f327-4aa7-a772-32a0b2bf59e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = torch.tensor([[0.43, 0.15, 0.89], #your\n",
    "                       [0.55, 0.87, 0.66], #journey\n",
    "                       [0.57, 0.85, 0.64], #starts\n",
    "                       [0.22, 0.58, 0.33], #with\n",
    "                       [0.77, 0.25, 0.10], #one\n",
    "                       [0.05, 0.80, 0.55]]) #step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5acf58cf-3305-4813-b340-18a7240d73a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.mplot3d import Axes3D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c058236-e7b6-4d96-91ce-a294ca6d7b85",
   "metadata": {},
   "outputs": [],
   "source": [
    "words = [\"Your\", \"journey\", \"srats\", \"with\", \"one\", \"step\"]\n",
    "\n",
    "x_coords = inputs[:, 0].numpy()\n",
    "y_coords = inputs[:, 1].numpy()\n",
    "z_coords = inputs[:, 2].numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4028180-cb3f-4408-bbec-97518738cef8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a4a6116-dd76-4f97-86a9-f7e4191f36c2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18461c4a-dea0-45d7-bc53-3c089140915e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f44bd261-ec0c-4090-983e-95280b82aec2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27b94085-4394-43dd-b53a-9009d75e2c54",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ebe2d82-2c4c-48ff-a933-0fd06ae2470f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
