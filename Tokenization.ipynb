{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6b8869b0-8e6f-4db6-b1dd-d9e05d51a75d",
   "metadata": {},
   "source": [
    "## Implemented two simple tokenizers from scratch and demonstrated tiktoken library.\n",
    "Dataset used: 'The verdict' by Edith Warton(1908)\n",
    "\n",
    "Implementing the first step of data prepration and sampling: Tokenization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6326149f-6b29-48d4-b131-7e702c4b5d9e",
   "metadata": {},
   "source": [
    "Step 1: Creating tokens (word based tokenizers) or tokenizing text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "11a459b0-59f5-4a80-a47a-2f54256e91b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"the-verdict.txt\", \"r\", encoding=\"utf-8\") as f:\n",
    "    raw_text = f.read()"
   ]
  },
  {
   "cell_type": "raw",
   "id": "384c8388-9074-49bb-a6cd-95ad5b691df2",
   "metadata": {},
   "source": [
    "print(\"Total number of character:\", len(raw_text))\n",
    "print(raw_text[100:450])\n",
    "print(type(raw_text))"
   ]
  },
  {
   "cell_type": "raw",
   "id": "7ea48426-a5fe-429a-a8d5-8ecb46ce17ad",
   "metadata": {},
   "source": [
    "import re\n",
    "\n",
    "preprocessed = re.split(r'([,.:;?_!\"()\\']|--|\\s)', raw_text)\n",
    "preprocessed = [item.strip() for item in preprocessed if item.strip()]\n",
    "\n",
    "print(preprocessed[:15])\n",
    "print(len(preprocessed))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "245db99e-6c1f-449d-8ec8-4f291bf616d3",
   "metadata": {},
   "source": [
    "Step 2: Creating Token IDs"
   ]
  },
  {
   "cell_type": "raw",
   "id": "c5380fc1-8b60-4742-9c2a-b3fe794147b8",
   "metadata": {},
   "source": [
    "all_words = sorted(set(preprocessed))\n",
    "vocab = {token:integer for integer,token in enumerate(all_words)}\n",
    "len(vocab)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05b2e09c-844d-4e3b-9e71-05d07edb1681",
   "metadata": {},
   "source": [
    "Implementing a python class for tokenization.\n",
    "This class will have two methods, encode and decode.\n",
    "\n",
    "Step 1: Store the vocabulary as class attribute for access in the encode and decode method.\n",
    "\n",
    "Step 2: Create an inverse vocabulary that maps token IDs back to the original text tokens.\n",
    "\n",
    "Step 3: process input text into token IDs\n",
    "\n",
    "Step 4: Convert token IDs back into text"
   ]
  },
  {
   "cell_type": "raw",
   "id": "38ae9041-05b9-4e73-9f37-5b70f15581e5",
   "metadata": {},
   "source": [
    "class SimpleTokenizerV1:\n",
    "    def __init__(self, vocab):\n",
    "        self.str_to_int = vocab\n",
    "        self.int_to_str = {t:s for s, t in vocab.items()}\n",
    "\n",
    "    def encode(self, text):\n",
    "        preprocessed = re.split(r'[,.:;?_!\"()\\']|--|\\s', text)\n",
    "        preprocessed = [item.strip() for item in preprocessed if item.strip()]\n",
    "        ids = [self.str_to_int[s] for s in preprocessed]\n",
    "        return ids\n",
    "\n",
    "    def decode(self, ids):\n",
    "        text = \" \".join([self.int_to_str[i] for i in ids])\n",
    "        text = re.sub(r'\\s+([,.?!\"()\\'])', r'\\1', text)\n",
    "        return text"
   ]
  },
  {
   "cell_type": "raw",
   "id": "b2c7f256-b054-4c4a-919a-f8a6a305c7c0",
   "metadata": {},
   "source": [
    "tokenizer = SimpleTokenizerV1(vocab)\n",
    "text = \"\"\"\"It's the last he painted, you know,\"\n",
    "           Mrs. Gisburn said with pardonable pride.\"\"\"\n",
    "ids = tokenizer.encode(text)\n",
    "print(ids)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "302b391b-630a-4ff8-ab5d-89e5795b26f9",
   "metadata": {},
   "source": [
    "tokenizer.decode(ids)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "55fe20b9-7cb8-4cfd-a944-a1300c7a5cd5",
   "metadata": {},
   "source": [
    "text = \"Hello, do you like tea?\"\n",
    "#print(tokenizer.encode(text))"
   ]
  },
  {
   "cell_type": "raw",
   "id": "379b3838-071e-4e9b-8d00-57fe785ed77a",
   "metadata": {},
   "source": [
    "all_tokens = sorted(set(preprocessed))\n",
    "all_tokens.extend([\"<|endoftext|>\", \"<|unk|>\"])\n",
    "vocab = {token:integer for integer,token in enumerate(all_tokens)}\n",
    "len(vocab)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "5df17a9c-bfa4-4e6b-bf5c-9341709269b0",
   "metadata": {},
   "source": [
    "for item in list(vocab.items())[-5:]:\n",
    "    print(item)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "8ad81191-ddea-4603-b9d7-4321c55e421a",
   "metadata": {},
   "source": [
    "class SimpleTokenizerV2:\n",
    "    def __init__(self, vocab):\n",
    "        self.toInt = vocab\n",
    "        self.toStr = {integer:token for token, integer in vocab.items()}\n",
    "\n",
    "    def encode(self, text):\n",
    "        preprocessed = re.split(r'([,.:;?_!()\\']|--|\\s)', text)\n",
    "        preprocessed = [item.strip() for item in preprocessed if item.strip()]\n",
    "        preprocessed = [item if item in self.toInt else \"<|unk|>\" for item in preprocessed]\n",
    "        ids = [self.toInt[s] for s in preprocessed]\n",
    "        return ids\n",
    "\n",
    "    def decode(self, ids):\n",
    "        text = \" \".join([self.toStr[i] for i in ids])\n",
    "        text = re.sub(r'\\s+([,.?!\"()\\'])', r'\\1', text)\n",
    "        return text\n"
   ]
  },
  {
   "cell_type": "raw",
   "id": "11aa1a6c-e816-467f-924a-118353dedbeb",
   "metadata": {},
   "source": [
    "tokenizer = SimpleTokenizerV2(vocab)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "bcaea710-e8ec-442e-b4e6-b0a807dd25fa",
   "metadata": {},
   "source": [
    "text1 = \"Hello, do you like tea?\"\n",
    "text2 = \"In the sunlit terraces of the palace.\"\n",
    "text = \" <|endoftext|> \".join((text1, text2))\n",
    "print('text: ', text)\n",
    "ids = tokenizer.encode(text)\n",
    "print(ids)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "7730b644-ce0b-495a-b273-51cfa1e3c252",
   "metadata": {},
   "source": [
    "tokenizer.decode(ids)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "117be90b-929b-4382-851f-d3626a5dd918",
   "metadata": {},
   "source": [
    "## BYTE PAIR ENCODING\n",
    "Implementing BPE from scratch can be relatively complicated, thus we will use an existing Python open-source library called tiktoken which is a fast BPE tokenizer for use wuth OPENAI's models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "afac4977-0770-4205-a8ec-404097571573",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tiktoken version:  0.8.0\n"
     ]
    }
   ],
   "source": [
    "import tiktoken\n",
    "import importlib\n",
    "print('tiktoken version: ', importlib.metadata.version(\"tiktoken\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5775dcc9-1ac5-4df1-afac-6884d050fe18",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = tiktoken.get_encoding(\"gpt2\")"
   ]
  },
  {
   "cell_type": "raw",
   "id": "88f949f7-8a93-4d66-b213-0ea990ae9919",
   "metadata": {},
   "source": [
    "text = ( \"Hello, do you like tea? <|endoftext|> In the sunlit terraces of someunknownPlace.\" )\n",
    "\n",
    "integers = tokenizer.encode(text, allowed_special={\"<|endoftext|>\"})\n",
    "print(integers)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "d38ee9b2-a109-4f32-840d-bd82802ea485",
   "metadata": {},
   "source": [
    "strings = tokenizer.decode(integers)\n",
    "print(strings)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b1850a7-5825-4365-9c29-77af4fda8dd2",
   "metadata": {},
   "source": [
    "The BPE tokenizer can handle unknown words. How can it achieve this without using <|unk|> token?\n",
    "\n",
    "\n",
    "The algorithm underlying BPE breaksdown words that aren't in its predefined vocabulary into smaller subword units or even individual characters.\n",
    "This enables it to handle out of vocabulary(OOV) words."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63e82bf2-56ea-4a75-8536-92edfab8027d",
   "metadata": {},
   "source": [
    "An example to illustrate how the BPE tokenizer deals with unknown tokens"
   ]
  },
  {
   "cell_type": "raw",
   "id": "9d4c8614-a710-4d56-9650-18e83e907e98",
   "metadata": {},
   "source": [
    "integers = tokenizer.encode(\"Akwirw ier\")\n",
    "print(\"integers: \", integers)\n",
    "\n",
    "strings = tokenizer.decode(integers)\n",
    "print(\"strings: \", strings)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf9d1f54-33d8-4ac4-8603-5656d5a1a04e",
   "metadata": {},
   "source": [
    "## Creating input-output layer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2ef66ff-2143-4cfb-bfe9-4e34f61c5a19",
   "metadata": {},
   "source": [
    "We implement a data loader that fetches the input-output pairs using a sliding window approach."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5d81f116-5756-45b1-aaf1-21781a91725c",
   "metadata": {},
   "outputs": [],
   "source": [
    "enc_text = tokenizer.encode(raw_text)\n",
    "enc_sample = enc_text[50:]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9586493-8563-4dc5-83dd-f20e7c7c1bdc",
   "metadata": {},
   "source": [
    "## Context size \n",
    "determines how many tokens are included in the input. The model istrained to look at a sequence of context_size number of words to predict the next word in the sequence.\n",
    "\n",
    "Each input-output pair contains context size number of prediction tasks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6aa5f2d8-99b9-4ab1-ba12-9406a324c852",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x:  [290, 4920, 2241, 287]\n",
      "y:    [4920, 2241, 287, 257]\n"
     ]
    }
   ],
   "source": [
    "context_size = 4\n",
    "x = enc_sample[:context_size]\n",
    "y = enc_sample[1:context_size+1]\n",
    "\n",
    "print(f\"x:  {x}\")\n",
    "print(f\"y:    {y}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fb71113f-70a8-4d2f-8f5e-1bb3fe73794f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[290] ----> 4920\n",
      "[290, 4920] ----> 2241\n",
      "[290, 4920, 2241] ----> 287\n",
      "[290, 4920, 2241, 287] ----> 257\n"
     ]
    }
   ],
   "source": [
    "for i in range(1, context_size+1):\n",
    "    context = enc_sample[:i]\n",
    "    desired = enc_sample[i]\n",
    "    print(context, \"---->\", desired)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "38d3e8ff-85b8-4633-ad52-83345ab9949f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " and ---->  established\n",
      " and established ---->  himself\n",
      " and established himself ---->  in\n",
      " and established himself in ---->  a\n"
     ]
    }
   ],
   "source": [
    "for i in range(1, context_size+1):\n",
    "    context = enc_sample[:i]\n",
    "    desired = enc_sample[i]\n",
    "    print(tokenizer.decode(context), \"---->\", tokenizer.decode([desired]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d47d118b-5301-4608-8bcd-5fb62d53d0f3",
   "metadata": {},
   "source": [
    "## Data Loader\n",
    "iterates over the input dataset and returns inputs and targets as PyTorch tensors.\n",
    "\n",
    "We implement dataloader using PyTorch datasets and dataloader classes.\n",
    "\n",
    "We aim at returning two tensors: an input tensor and an output tensor.\n",
    "\n",
    "Helps us do parallel processing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c7425667-1f25-40ef-9f3d-2ce2faf7d021",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "class GPTDatasetV1(Dataset):\n",
    "    def __init__(self, txt, tokenizer, max_length, stride):\n",
    "        self.input_ids = []\n",
    "        self.target_ids = []\n",
    "        self.token_ids = tokenizer.encode(txt, allowed_special={\"<|endoftext|>\"})\n",
    "\n",
    "        for i in range(0, len(self.token_ids)-max_length, stride):\n",
    "            input_chunk = self.token_ids[i : max_length+i]\n",
    "            target_chunk = self.token_ids[i+1 : i + max_length + 1]\n",
    "            self.input_ids.append(torch.tensor(input_chunk))\n",
    "            self.target_ids.append(torch.tensor(target_chunk))\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.input_ids)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.input_ids[idx], self.target_ids[idx] #idx=index"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0296a620-cdf1-449f-ac73-29e49c71c629",
   "metadata": {},
   "source": [
    "drop_last=true = drops the last batchif it is shorter than the specified batch_size to prevent loss spikes during training.\n",
    "\n",
    "batch_size = how many batches or CPU processes we want to run parallelly\n",
    "\n",
    "max_length = context length\n",
    "\n",
    "num_workers = number of CPU threads which we can run simultaneously. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "212c711e-807d-446a-a6f2-05051cb15ed5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_dataloader_v1(txt, batch_size=4, max_length=256, stride=128, shuffle=True, drop_last=True, num_workers=0):\n",
    "    tokenizer = tiktoken.get_encoding(\"o200k_base\")\n",
    "    dataset = GPTDatasetV1(txt, tokenizer, max_length, stride) #creating dataset\n",
    "    dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=shuffle, drop_last=drop_last, num_workers=num_workers)\n",
    "    return dataloader\n",
    "\n",
    "# this function governs batch processing or the parallel processing we need which is governed by the batch size.\n",
    "#It help us create the input output data pairs from the dataset which we defined earlier.   "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8dcdec02-b511-4df8-a1da-1e0e6eeaa1e9",
   "metadata": {},
   "source": [
    "We now convert the dataloader to python iterator to fetch the next entry via python built-in next() function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "be00c565-6230-4476-bc78-f1954761b76a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyTorch Version:  2.5.1+cpu\n",
      "Inputs:  tensor([[    40, 148954,   3324,   4525],\n",
      "        [ 10874, 165003,  33750,   7542],\n",
      "        [   261,  12424,  59245,    375],\n",
      "        [  6460,    261,   1899,  19807],\n",
      "        [  4951,    375,    786,    480],\n",
      "        [   673,    860,   2212,  19005],\n",
      "        [   316,    668,    316,   9598],\n",
      "        [   484,     11,    306,    290]])\n",
      "targets:  tensor([[148954,   3324,   4525,  10874],\n",
      "        [165003,  33750,   7542,    261],\n",
      "        [ 12424,  59245,    375,   6460],\n",
      "        [   261,   1899,  19807,   4951],\n",
      "        [   375,    786,    480,    673],\n",
      "        [   860,   2212,  19005,    316],\n",
      "        [   668,    316,   9598,    484],\n",
      "        [    11,    306,    290,   4679]])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "print(\"PyTorch Version: \", torch.__version__)\n",
    "dataloader = create_dataloader_v1(raw_text, batch_size=8, max_length=4, stride=4, shuffle=False)\n",
    "data_iter = iter(dataloader)\n",
    "inputs, targets = next(data_iter)\n",
    "print(\"Inputs: \", inputs)\n",
    "print(\"targets: \", targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "f71efdd0-9b2b-4a75-b1f8-b7e81dc5b22f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[tensor([[148954,   3324,   4525,  10874]]), tensor([[  3324,   4525,  10874, 165003]])]\n"
     ]
    }
   ],
   "source": [
    "second_batch = next(data_iter)\n",
    "print(second_batch)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f747d6cf-d312-4844-a4a0-e3d392f77570",
   "metadata": {},
   "source": [
    "Batch size of 1 are used for illustration puposes. Small batch sizes require less memort during training but lead to more noisy model updates.\n",
    "\n",
    "Batch size is a trade-off and hyperparameter to experiment with when training LLMs.\n",
    "\n",
    "Model will procces one batch before making the parameter updates."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c8b632d-2d15-4e1f-9574-e5b0b33e7e44",
   "metadata": {},
   "source": [
    "## TOKEN EMBEDDINGS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7afcbf57-45ef-401c-b7d9-d4bf7375c5eb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cfd22aa-d7cc-40d2-b265-8d0998630baf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc01a402-b753-4c8b-a1b8-7a343d903a81",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f0cf73a-39b1-4838-ae44-138d44688cda",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef9d9a39-b52e-4054-a225-a767ee092836",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "088fd92c-34a0-4558-ad9d-a03cde8ff8cb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57affa97-ab0c-489e-b842-3041ff29b89a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c077286-8990-4147-b786-ac68660cbafc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ac249f9-0622-425d-bac9-8f8d3f3712a3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c471964-e2a1-46df-8450-a16464d0a7b1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb3a479c-40db-4170-a424-862d5d8ffc63",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "caf8b3b9-b75c-4ed4-aa49-38fe5edeeab9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d0bd2e7-27be-4d0f-b02b-6091520a93e0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9198a1c4-3b04-478d-95e0-2772c38b8a91",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b93848d-4047-4bb4-ba81-f42d44f162f1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ba10a32-cda2-44ff-b774-702651b1a515",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb943f43-f327-4aa7-a772-32a0b2bf59e3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b50485a7-af6d-47c9-8944-2c73af94398d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c058236-e7b6-4d96-91ce-a294ca6d7b85",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4028180-cb3f-4408-bbec-97518738cef8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ebe2d82-2c4c-48ff-a933-0fd06ae2470f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4706d326-bb58-442b-9ce4-ca01232b1b25",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a942e31c-be39-4a52-9d94-961c4486989d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a22a07d5-35c9-4889-b9ab-88886237bd3b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0843c59b-73da-478a-98e7-1560050fb91c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0b08236-6be3-4b42-896c-e979b2d35270",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b74015b-3e14-4243-b911-e01a0adc9dfc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3add55ce-0c50-42e8-857b-2ed858b974d7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9921dfa-ca1a-4796-8a8b-8f34f5218f69",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ba42dba-2113-4f7a-be30-343988401fe2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ebf8a8d-9f0f-447b-b0f6-86eeeba8ea54",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e884e515-263d-4652-85e5-65c9ad13d538",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a90b43a3-1a72-4c17-a348-3be3baf0aa78",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5025c568-8ed4-425b-b443-50ff7a3a4437",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3888856-51b6-4b33-8dfe-e7caee6d6d80",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbc0383a-99ff-4bf6-b044-a62284ea7169",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49b25336-6e1a-43d1-9683-ff91bd5186c2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d498bd0-3d41-439e-9f63-c32ed9a50dfa",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "861b50bb-27ad-40de-a1d3-754d0a05280c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2e621eb-4dd6-4546-b3a0-d0ac2d1d7a1d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a7d4064-4903-416f-9fb2-3e1e6f3a8a86",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8db529cb-89bf-4809-a0ab-5f2d15e5cfd5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cfeaf5f-3564-470d-b150-ac5ec9086825",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a8d1559-9704-4bb4-9a1d-ca5d2cc83962",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e23d7ca-682d-4e05-8917-b3b822c31cca",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb2046d2-dbdf-4438-82b8-f88b0c3f9828",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "264b12d8-b39b-4965-adf2-4aa908ab6e5c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86c8213c-aba9-4fee-b7fa-8a9ee219f886",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e5e50b0-e7b5-4359-a52d-c5ba8fee9159",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99798b78-e005-4880-be57-6fe8a4960863",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da8460e7-69e7-49af-b840-2183e1c47ff8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc3196d1-490d-4c10-9484-4f6cd450505a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "532ecc11-4e50-4bc5-91d8-0787f3535e1b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b2303fe-c435-477c-86b0-d262f5cd606c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd24f87b-3243-4f0b-8117-01a414bf8c18",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e508fda-e70a-4d3a-b20b-7a9d7c90d4b2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abb650db-1e0e-4eb1-9b7f-6064157c741e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa2d33ee-1990-40d8-bc4a-ae4d999bfb6b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ad96a27-1190-4e77-bced-afb6c45b5828",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e4ae173-7953-4524-bc18-0bb57741256e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25ff947c-9a81-4f5e-95dd-bff795794960",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fa3b9a6-9ec2-46c1-a0b8-c6e9afc1279a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5d3efb4-8e1b-4f64-96e5-97f3d94922b7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2922834a-8d71-4e89-a7e4-3c125d54e954",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1354dcb-abe7-4f92-8f3c-daa8bb047156",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
