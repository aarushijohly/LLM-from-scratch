{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4f6dad8d-bea1-4282-80fd-b8a5ceab7e34",
   "metadata": {},
   "source": [
    "## Implemented two simple tokenizers from scratch and demonstrated tiktoken library.\n",
    "Dataset used: 'The verdict' by Edith Warton(1908)\n",
    "Embedding: The process of converting data into a vector format.\n",
    "Implementing the first step of data prepration and sampling: Tokenization.\n",
    "\n",
    "Step 1: Creating tokens (word based tokenizers) or tokenizing text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8a0c70e-4e02-494f-9d3e-8f14b31f47a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import urllib.request"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "54c86a97-48d3-4b07-a7e5-4418a0895898",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"the-verdict.txt\", \"r\", encoding=\"utf-8\") as f:\n",
    "    raw_text = f.read()"
   ]
  },
  {
   "cell_type": "raw",
   "id": "fe541346-0263-4ea8-b0a6-7be8c6446b47",
   "metadata": {},
   "source": [
    "import re\n",
    "preprocessed = re.split(r'([,.:;?_!\"()\\']| |--|\\\\s)', raw_text)\n",
    "preprocessed = [item.strip() for item in preprocessed if item.strip()]\n",
    "print(preprocessed[:10])\n",
    "\n",
    "all_words = sorted(set(preprocessed))\n",
    "vocab_size = len(all_words)\n",
    "vocab = {token:integer for integer,token in enumerate(all_words)}\n",
    "print(f\"Vocabulary size: {vocab_size}\")"
   ]
  },
  {
   "cell_type": "raw",
   "id": "bab2eb06-917b-4155-a823-518204a60830",
   "metadata": {},
   "source": [
    "class SimpleTokenizerV1:\n",
    "    def __init__(self, vocab):\n",
    "        self.str_to_int = vocab\n",
    "        self.int_to_str = {t:s for s, t in vocab.items()}\n",
    "\n",
    "    def encode(self, text):\n",
    "        preprocessed = re.split(r'[,.:;?_!\"()\\']|--|\\s', text)\n",
    "        preprocessed = [item.strip() for item in preprocessed if item.strip()]\n",
    "        ids = [self.str_to_int[s] for s in preprocessed]\n",
    "        return ids\n",
    "\n",
    "    def decode(self, ids):\n",
    "        text = \" \".join([self.int_to_str[i] for i in ids])\n",
    "        text = re.sub(r'\\s+([,.?!\"()\\'])', r'\\1', text)\n",
    "        return text"
   ]
  },
  {
   "cell_type": "raw",
   "id": "b41b727b-b36e-4e07-b146-994963c47c27",
   "metadata": {},
   "source": [
    "tokenizer = SimpleTokenizerV1(vocab)\n",
    "text = \"\"\"\"It's the last he painted, you know,\"\n",
    "        Mrs. Gisburn said with pardonable pride.\"\"\"\n",
    "ids = tokenizer.encode(text)\n",
    "print(ids)\n",
    "\n",
    "tokenizer.decode(ids)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "3cd76184-275a-4606-aedf-86addbf6bf76",
   "metadata": {},
   "source": [
    "text = \"Hello, do you like tea?\"\n",
    "#print(tokenizer.encode(text))"
   ]
  },
  {
   "cell_type": "raw",
   "id": "110443ab-31cf-4b45-8642-5f82ac81925e",
   "metadata": {},
   "source": [
    "all_tokens = sorted(set(preprocessed))\n",
    "all_tokens.extend([\"<|endoftext|>\", \"<|unk|>\"])\n",
    "vocab = {token:integer for integer,token in enumerate(all_tokens)}\n",
    "len(vocab)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "4591a0ff-3478-4b06-9d75-11b84945c145",
   "metadata": {},
   "source": [
    "class SimpleTokenizerV2:\n",
    "    def __init__(self, vocab):\n",
    "        self.toInt = vocab\n",
    "        self.toStr = {integer:token for token, integer in vocab.items()}\n",
    "\n",
    "    def encode(self, text):\n",
    "        preprocessed = re.split(r'([,.:;?_!()\\']|--|\\s)', text)\n",
    "        preprocessed = [item.strip() for item in preprocessed if item.strip()]\n",
    "        preprocessed = [item if item in self.toInt else \"<|unk|>\" for item in preprocessed]\n",
    "        ids = [self.toInt[s] for s in preprocessed]\n",
    "        return ids\n",
    "\n",
    "    def decode(self, ids):\n",
    "        text = \" \".join([self.toStr[i] for i in ids])\n",
    "        text = re.sub(r'\\s+([,.?!\"()\\'])', r'\\1', text)\n",
    "        return text"
   ]
  },
  {
   "cell_type": "raw",
   "id": "f12167c3-d111-4bed-be9f-2a5fb539d435",
   "metadata": {},
   "source": [
    "tokenizer = SimpleTokenizerV2(vocab)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "85359768-f339-4c1b-8e54-1c10e4b1956a",
   "metadata": {},
   "source": [
    "text1 = \"Hello, do you like tea?\"\n",
    "text2 = \"In the sunlit terraces of the palace.\"\n",
    "text = \" <|endoftext|> \".join((text1, text2))\n",
    "print('text: ', text)\n",
    "ids = tokenizer.encode(text)\n",
    "print(ids)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "04571cef-a74b-499d-b3ff-93e85052b635",
   "metadata": {},
   "source": [
    "tokenizer.decode(ids)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "117be90b-929b-4382-851f-d3626a5dd918",
   "metadata": {},
   "source": [
    "## BYTE PAIR ENCODING\n",
    "Implementing BPE from scratch can be relatively complicated, thus we will use an existing Python open-source library called tiktoken which is a fast BPE tokenizer for use wuth OPENAI's models.\n",
    "\n",
    "Encode: Tokenization and conversion into token IDs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "afac4977-0770-4205-a8ec-404097571573",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tiktoken version:  0.9.0\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import tiktoken\n",
    "import importlib\n",
    "print('tiktoken version: ', importlib.metadata.version(\"tiktoken\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5775dcc9-1ac5-4df1-afac-6884d050fe18",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = tiktoken.get_encoding(\"gpt2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "654b70dd-8292-4951-ae05-bafe36313bf8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[15496, 11, 466, 345, 588, 8887, 30, 220, 50256, 554, 262, 4252, 18250, 8812, 2114, 286, 617, 34680, 27271, 13]\n"
     ]
    }
   ],
   "source": [
    "text = ( \"Hello, do you like tea? <|endoftext|> In the sunlit terraces of someunknownPlace.\" )\n",
    "\n",
    "integers = tokenizer.encode(text, allowed_special={\"<|endoftext|>\"})\n",
    "print(integers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2d80a41c-654c-4bab-9674-d743653faf38",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello, do you like tea? <|endoftext|> In the sunlit terraces of someunknownPlace.\n"
     ]
    }
   ],
   "source": [
    "strings = tokenizer.decode(integers)\n",
    "print(strings)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b1850a7-5825-4365-9c29-77af4fda8dd2",
   "metadata": {},
   "source": [
    "The BPE tokenizer can handle unknown words. How can it achieve this without using <|unk|> token?\n",
    "\n",
    "\n",
    "The algorithm underlying BPE breaksdown words that aren't in its predefined vocabulary into smaller subword units or even individual characters.\n",
    "This enables it to handle out of vocabulary(OOV) words.\n",
    "\n",
    "An example to illustrate how the BPE tokenizer deals with unknown tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "58497384-078d-481d-b712-96fe37070a1b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "integers: [33901, 86, 343, 86, 220, 959]\n",
      "type: <class 'list'>\n",
      "strings: Akwirw ier\n"
     ]
    }
   ],
   "source": [
    "integers = tokenizer.encode(\"Akwirw ier\")\n",
    "print(f\"integers: {integers}\")\n",
    "print(f\"type: {type(integers)}\")\n",
    "\n",
    "strings = tokenizer.decode(integers)\n",
    "print(f\"strings: {strings}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf9d1f54-33d8-4ac4-8603-5656d5a1a04e",
   "metadata": {},
   "source": [
    "## Creating input-output layer\n",
    "\n",
    "We implement a data loader that fetches the input-output pairs using a sliding window approach."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5d81f116-5756-45b1-aaf1-21781a91725c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of enc_text: 5145\n"
     ]
    }
   ],
   "source": [
    "enc_text = tokenizer.encode(raw_text)\n",
    "print(f\"Length of enc_text: {len(enc_text)}\")\n",
    "enc_sample = enc_text[50:]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9586493-8563-4dc5-83dd-f20e7c7c1bdc",
   "metadata": {},
   "source": [
    "## Context size \n",
    "Determines how many tokens are included in the input. The model istrained to look at a sequence of context_size number of words to predict the next word in the sequence.\n",
    "\n",
    "Each input-output pair contains context size number of prediction tasks."
   ]
  },
  {
   "cell_type": "raw",
   "id": "0340bf47-4fb1-4ecf-bdfa-d90cc07993d9",
   "metadata": {},
   "source": [
    "context_size = 4\n",
    "x = enc_sample[:context_size]\n",
    "y = enc_sample[1:context_size+1]\n",
    "\n",
    "print(f\"x:  {x}\")\n",
    "print(f\"y:    {y}\")"
   ]
  },
  {
   "cell_type": "raw",
   "id": "9f36098e-5e7f-4c2e-9a27-59d1a4cb741b",
   "metadata": {},
   "source": [
    "for i in range(1, context_size+1):\n",
    "    context = enc_sample[:i]\n",
    "    desired = enc_sample[i]\n",
    "    print(context, \"---->\", desired)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "c945c5eb-99e5-42aa-8f5f-bf2b3dfff842",
   "metadata": {},
   "source": [
    "for i in range(1, context_size+1):\n",
    "    context = enc_sample[:i]\n",
    "    desired = enc_sample[i]\n",
    "    print(tokenizer.decode(context), \"---->\", tokenizer.decode([desired]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d47d118b-5301-4608-8bcd-5fb62d53d0f3",
   "metadata": {},
   "source": [
    "## Data Loader\n",
    "Iterates over the input dataset and returns inputs and targets as PyTorch tensors. We are iterested in returning two tensors: an input tensor containing text that the LLM sees and a target tensor that includes the trget for LLM to predict. We implement dataloader using PyTorch datasets and dataloader classes. We aim at returning two tensors: an input tensor and an output tensor. Helps us do parallel processing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c7425667-1f25-40ef-9f3d-2ce2faf7d021",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "class GPTDatasetV1(Dataset):\n",
    "    def __init__(self, txt, tokenizer, max_length, stride):\n",
    "        self.input_ids = []\n",
    "        self.target_ids = []\n",
    "        self.token_ids = tokenizer.encode(txt, allowed_special={\"<|endoftext|>\"})\n",
    "\n",
    "        for i in range(0, len(self.token_ids)-max_length, stride):\n",
    "            input_chunk = self.token_ids[i : max_length+i]\n",
    "            target_chunk = self.token_ids[i+1 : i + max_length + 1]\n",
    "            self.input_ids.append(torch.tensor(input_chunk))\n",
    "            self.target_ids.append(torch.tensor(target_chunk))\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.input_ids)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.input_ids[idx], self.target_ids[idx] #idx=index"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0296a620-cdf1-449f-ac73-29e49c71c629",
   "metadata": {},
   "source": [
    "drop_last=true: drops the last batch if it is shorter than the specified batch_size to prevent loss spikes during training.\n",
    "\n",
    "batch_size = how many batches or CPU processes we want to run parallelly\n",
    "\n",
    "max_length = context length\n",
    "\n",
    "num_workers = number of CPU threads which we can run simultaneously. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "212c711e-807d-446a-a6f2-05051cb15ed5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_dataloader_v1(txt, batch_size=4, max_length=256, stride=128, shuffle=True, drop_last=True, num_workers=0):\n",
    "    tokenizer = tiktoken.get_encoding(\"o200k_base\")\n",
    "    dataset = GPTDatasetV1(txt, tokenizer, max_length, stride) #creating dataset\n",
    "    dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=shuffle, drop_last=drop_last, num_workers=num_workers)\n",
    "    return dataloader\n",
    "\n",
    "#this code uses the GPTDatasetV1 to load the inputs in batches via a PyTorch DataLoader.\n",
    "#this function governs batch processing or the parallel processing we need which is governed by the batch size.\n",
    "#It help us create the input output data pairs from the dataset which we defined earlier.   "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8dcdec02-b511-4df8-a1da-1e0e6eeaa1e9",
   "metadata": {},
   "source": [
    "We now convert the dataloader to python iterator to fetch the next entry via python built-in next() function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "be00c565-6230-4476-bc78-f1954761b76a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyTorch Version:  2.6.0\n",
      "Inputs:  tensor([[    40, 148954,   3324,   4525],\n",
      "        [ 10874, 165003,  33750,   7542],\n",
      "        [   261,  12424,  59245,    375],\n",
      "        [  6460,    261,   1899,  19807],\n",
      "        [  4951,    375,    786,    480],\n",
      "        [   673,    860,   2212,  19005],\n",
      "        [   316,    668,    316,   9598],\n",
      "        [   484,     11,    306,    290]])\n",
      "targets:  tensor([[148954,   3324,   4525,  10874],\n",
      "        [165003,  33750,   7542,    261],\n",
      "        [ 12424,  59245,    375,   6460],\n",
      "        [   261,   1899,  19807,   4951],\n",
      "        [   375,    786,    480,    673],\n",
      "        [   860,   2212,  19005,    316],\n",
      "        [   668,    316,   9598,    484],\n",
      "        [    11,    306,    290,   4679]])\n"
     ]
    }
   ],
   "source": [
    "print(\"PyTorch Version: \", torch.__version__)\n",
    "dataloader = create_dataloader_v1(raw_text, batch_size=8, max_length=4, stride=4, shuffle=False)\n",
    "data_iter = iter(dataloader)\n",
    "inputs, targets = next(data_iter)\n",
    "print(\"Inputs: \", inputs)\n",
    "print(\"targets: \", targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f71efdd0-9b2b-4a75-b1f8-b7e81dc5b22f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[tensor([[  4679,    328,   1232,  40373],\n",
      "        [    11,    501,   1458,  22664],\n",
      "        [  1232,  21352,     11,  17189],\n",
      "        [   261,  10358, 101819,     11],\n",
      "        [   326,  12812,  11166,    306],\n",
      "        [   261,  38350,    402,    290],\n",
      "        [123397,     13,    350,  52861],\n",
      "        [   357,   7542,   4525,    480]]), tensor([[   328,   1232,  40373,     11],\n",
      "        [   501,   1458,  22664,   1232],\n",
      "        [ 21352,     11,  17189,    261],\n",
      "        [ 10358, 101819,     11,    326],\n",
      "        [ 12812,  11166,    306,    261],\n",
      "        [ 38350,    402,    290, 123397],\n",
      "        [    13,    350,  52861,    357],\n",
      "        [  7542,   4525,    480,   1481]])]\n"
     ]
    }
   ],
   "source": [
    "second_batch = next(data_iter)\n",
    "print(second_batch)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f747d6cf-d312-4844-a4a0-e3d392f77570",
   "metadata": {},
   "source": [
    "Small batch sizes require less memory during training but lead to more noisy model updates. Batch size is a trade-off and hyperparameter to experiment with when training LLMs. Model will procces one batch before making the parameter updates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0f56cc7-3e7a-48f4-aead-ec954c21863b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "968b9343-ff32-4a30-b13f-22d793611f79",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19c7f0b5-ae11-4276-8569-3b2b78c096e3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9e41cec-136b-4e17-ae4e-6854a401d4f1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dabc6eea-333d-4a5e-b03e-c21ec366d86c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
