{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6b8869b0-8e6f-4db6-b1dd-d9e05d51a75d",
   "metadata": {},
   "source": [
    "Implemented two simple tokenizers from scratch and demonstrated tiktoken library.\n",
    "\n",
    "\n",
    "Dataset used: 'The verdict' by Edith Warton(1908)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6326149f-6b29-48d4-b131-7e702c4b5d9e",
   "metadata": {},
   "source": [
    "Step 1: Creating tokens (word based tokenizers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "11a459b0-59f5-4a80-a47a-2f54256e91b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"the-verdict.txt\", \"r\", encoding=\"utf-8\") as f:\n",
    "    raw_text = f.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b38f189b-e720-4a3f-a67d-64d0b3b93b56",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of character: 20479\n",
      "I HAD always thought Jack Gisburn rather a cheap genius--though a good fellow enough--so it was no \n",
      "<class 'str'>\n"
     ]
    }
   ],
   "source": [
    "print(\"Total number of character:\", len(raw_text))\n",
    "print(raw_text[:99])\n",
    "print(type(raw_text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3d7e4561-9ba7-447a-a17f-ce535ab43576",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['I', 'HAD', 'always', 'thought', 'Jack', 'Gisburn', 'rather', 'a', 'cheap', 'genius', '--', 'though', 'a', 'good', 'fellow']\n",
      "4690\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "preprocessed = re.split(r'([,.:;?_!\"()\\']|--|\\s)', raw_text)\n",
    "preprocessed = [item.strip() for item in preprocessed if item.strip()]\n",
    "\n",
    "print(preprocessed[:15])\n",
    "print(len(preprocessed))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "245db99e-6c1f-449d-8ec8-4f291bf616d3",
   "metadata": {},
   "source": [
    "Step 2: Creating Token IDs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c5f775bf-e8d7-4c85-be60-439f0f773248",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1130"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_words = sorted(set(preprocessed))\n",
    "vocab = {token:integer for integer,token in enumerate(all_words)}\n",
    "len(vocab)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05b2e09c-844d-4e3b-9e71-05d07edb1681",
   "metadata": {},
   "source": [
    "Implementing a python class for tokenization.\n",
    "This class will have two methods, encode and decode.\n",
    "Step 1: Store the vocabulary as class attribute for access in the encode and decode method.\n",
    "\n",
    "Step 2: Create an inverse vocabulary that maps token IDs back to the original text tokens.\n",
    "\n",
    "Step 3: process input text into token IDs\n",
    "\n",
    "Step 4: Convert token IDs back into text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7f223aeb-b2ab-424a-9c47-e5d859e255e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleTokenizerV1:\n",
    "    def __init__(self, vocab):\n",
    "        #self.toInt = vocab\n",
    "        self.toStr = {t:s for s, t in vocab.items()}\n",
    "\n",
    "    def encode(self, text):\n",
    "        preprocessed = re.split(r'[,.:;?_!\"()\\']|--|\\s', text)\n",
    "        preprocessed = [item.strip() for item in preprocessed if item.strip()]\n",
    "        ids = [vocab[s] for s in preprocessed]\n",
    "        return ids\n",
    "\n",
    "    def decode(self, ids):\n",
    "        text = \" \".join([self.toStr[i] for i in ids])\n",
    "        text = re.sub(r'\\s+([,.?!\"()\\'])', r'\\1', text)\n",
    "        return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "24562335-68c2-418c-a0ab-c61be98c2e6b",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'vocab' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[2], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m tokenizer \u001b[38;5;241m=\u001b[39m SimpleTokenizerV1(\u001b[43mvocab\u001b[49m)\n\u001b[0;32m      2\u001b[0m text \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\"\"\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIt\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124ms the last he painted, you know,\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;124m           Mrs. Gisburn said with pardonable pride.\u001b[39m\u001b[38;5;124m\"\"\"\u001b[39m\n\u001b[0;32m      4\u001b[0m ids \u001b[38;5;241m=\u001b[39m tokenizer\u001b[38;5;241m.\u001b[39mencode(text)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'vocab' is not defined"
     ]
    }
   ],
   "source": [
    "tokenizer = SimpleTokenizerV1(vocab)\n",
    "text = \"\"\"\"It's the last he painted, you know,\"\n",
    "           Mrs. Gisburn said with pardonable pride.\"\"\"\n",
    "ids = tokenizer.encode(text)\n",
    "print(ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8afca925-2d4b-48d3-8e28-ccce24d8e679",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'It s the last he painted you know Mrs Gisburn said with pardonable pride'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode(ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7106b9b8-a8b3-4875-ba25-496de99e4cb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"Hello, do you like tea?\"\n",
    "#print(tokenizer.encode(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7f1f3eaf-efff-476d-a181-7f0343a0a0fc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1132"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_tokens = sorted(set(preprocessed))\n",
    "all_tokens.extend([\"<|endoftext|>\", \"<|unk|>\"])\n",
    "vocab = {token:integer for integer,token in enumerate(all_tokens)}\n",
    "len(vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "70ec9f79-dd72-4358-8c20-c077544e4e3f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('younger', 1127)\n",
      "('your', 1128)\n",
      "('yourself', 1129)\n",
      "('<|endoftext|>', 1130)\n",
      "('<|unk|>', 1131)\n"
     ]
    }
   ],
   "source": [
    "for item in list(vocab.items())[-5:]:\n",
    "    print(item)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b0b32256-8f59-428f-8a47-2f6af84f9f0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleTokenizerV2:\n",
    "    def __init__(self, vocab):\n",
    "        self.toInt = vocab\n",
    "        self.toStr = {integer:token for token, integer in vocab.items()}\n",
    "\n",
    "    def encode(self, text):\n",
    "        preprocessed = re.split(r'([,.:;?_!()\\']|--|\\s)', text)\n",
    "        preprocessed = [item.strip() for item in preprocessed if item.strip()]\n",
    "        preprocessed = [item if item in self.toInt else \"<|unk|>\" for item in preprocessed]\n",
    "        ids = [self.toInt[s] for s in preprocessed]\n",
    "        return ids\n",
    "\n",
    "    def decode(self, ids):\n",
    "        text = \" \".join([self.toStr[i] for i in ids])\n",
    "        text = re.sub(r'\\s+([,.?!\"()\\'])', r'\\1', text)\n",
    "        return text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "988e9fa4-c3b8-45ef-a70e-d3dd520bb0f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = SimpleTokenizerV2(vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "5baeb243-ca14-4a3d-8130-a8f01ed329e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1131, 5, 355, 1126, 628, 975, 10, 1130, 55, 988, 956, 984, 722, 988, 1131, 7]\n"
     ]
    }
   ],
   "source": [
    "text1 = \"Hello, do you like tea?\"\n",
    "text2 = \"In the sunlit terraces of the palace.\"\n",
    "text = \" <|endoftext|> \".join((text1, text2))\n",
    "ids = tokenizer.encode(text)\n",
    "print(ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "99f920b0-08e7-4687-a235-723494fc3aa6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<|unk|>, do you like tea? <|endoftext|> In the sunlit terraces of the <|unk|>.'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode(ids)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "117be90b-929b-4382-851f-d3626a5dd918",
   "metadata": {},
   "source": [
    "BYTE PAIR ENCODING: Implementing BPE from scratch can be relatively complicated, thus we will use an existing Python open-source library called tiktoken."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "afac4977-0770-4205-a8ec-404097571573",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tiktoken\n",
    "import importlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "5775dcc9-1ac5-4df1-afac-6884d050fe18",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = tiktoken.get_encoding(\"gpt2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "e8345b58-581c-45bb-bc86-28072cdadbdd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[15496, 11, 466, 345, 588, 8887, 30, 220, 50256, 554, 262, 4252, 18250, 8812, 2114, 286, 617, 34680, 27271, 13]\n"
     ]
    }
   ],
   "source": [
    "text = ( \"Hello, do you like tea? <|endoftext|> In the sunlit terraces of someunknownPlace.\" )\n",
    "\n",
    "integers = tokenizer.encode(text, allowed_special={\"<|endoftext|>\"})\n",
    "print(integers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "cd686695-d926-4b05-9507-aa247e55f789",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello, do you like tea? <|endoftext|> In the sunlit terraces of someunknownPlace.\n"
     ]
    }
   ],
   "source": [
    "strings = tokenizer.decode(integers)\n",
    "print(strings)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b1850a7-5825-4365-9c29-77af4fda8dd2",
   "metadata": {},
   "source": [
    "The BPE tokenizer can handle unknown words. How can it achieve this without using <|unk|> token?\n",
    "\n",
    "\n",
    "The algorithm underlying BPE breaksdown words that aren't in its predefined vocabulary into smaller subword units or even individual characters.\n",
    "This enables it to handle out of vocabulary(OOV) words."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63e82bf2-56ea-4a75-8536-92edfab8027d",
   "metadata": {},
   "source": [
    "An example to illustrate how the BPE tokenizer deals with unknown tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "2a604a8d-664c-429d-908f-e295d7e1d384",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "integers:  [33901, 86, 343, 86, 220, 959]\n",
      "strings:  Akwirw ier\n"
     ]
    }
   ],
   "source": [
    "integers = tokenizer.encode(\"Akwirw ier\")\n",
    "print(\"integers: \", integers)\n",
    "\n",
    "strings = tokenizer.decode(integers)\n",
    "print(\"strings: \", strings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2922834a-8d71-4e89-a7e4-3c125d54e954",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1354dcb-abe7-4f92-8f3c-daa8bb047156",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
