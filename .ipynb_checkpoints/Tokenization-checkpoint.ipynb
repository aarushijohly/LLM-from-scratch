{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6b8869b0-8e6f-4db6-b1dd-d9e05d51a75d",
   "metadata": {},
   "source": [
    "## Implemented two simple tokenizers from scratch and demonstrated tiktoken library.\n",
    "Dataset used: 'The verdict' by Edith Warton(1908)\n",
    "Embedding: The process of converting data into a vector format.\n",
    "Implementing the first step of data prepration and sampling: Tokenization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6326149f-6b29-48d4-b131-7e702c4b5d9e",
   "metadata": {},
   "source": [
    "Step 1: Creating tokens (word based tokenizers) or tokenizing text"
   ]
  },
  {
   "cell_type": "raw",
   "id": "4abd215e-a7c3-485b-8772-d00d2043b5e0",
   "metadata": {},
   "source": [
    "with open(\"the-verdict.txt\", \"r\", encoding=\"utf-8\") as f:\n",
    "    raw_text = f.read()"
   ]
  },
  {
   "cell_type": "raw",
   "id": "fe541346-0263-4ea8-b0a6-7be8c6446b47",
   "metadata": {},
   "source": [
    "import re\n",
    "preprocessed = re.split(r'([,.:;?_!\"()\\']| |--|\\\\s)', raw_text)\n",
    "preprocessed = [item.strip() for item in preprocessed if item.strip()]\n",
    "print(preprocessed[:10])\n",
    "\n",
    "all_words = sorted(set(preprocessed))\n",
    "vocab_size = len(all_words)\n",
    "vocab = {token:integer for integer,token in enumerate(all_words)}\n",
    "print(f\"Vocabulary size: {vocab_size}\")"
   ]
  },
  {
   "cell_type": "raw",
   "id": "bab2eb06-917b-4155-a823-518204a60830",
   "metadata": {},
   "source": [
    "class SimpleTokenizerV1:\n",
    "    def __init__(self, vocab):\n",
    "        self.str_to_int = vocab\n",
    "        self.int_to_str = {t:s for s, t in vocab.items()}\n",
    "\n",
    "    def encode(self, text):\n",
    "        preprocessed = re.split(r'[,.:;?_!\"()\\']|--|\\s', text)\n",
    "        preprocessed = [item.strip() for item in preprocessed if item.strip()]\n",
    "        ids = [self.str_to_int[s] for s in preprocessed]\n",
    "        return ids\n",
    "\n",
    "    def decode(self, ids):\n",
    "        text = \" \".join([self.int_to_str[i] for i in ids])\n",
    "        text = re.sub(r'\\s+([,.?!\"()\\'])', r'\\1', text)\n",
    "        return text"
   ]
  },
  {
   "cell_type": "raw",
   "id": "b41b727b-b36e-4e07-b146-994963c47c27",
   "metadata": {},
   "source": [
    "tokenizer = SimpleTokenizerV1(vocab)\n",
    "text = \"\"\"\"It's the last he painted, you know,\"\n",
    "        Mrs. Gisburn said with pardonable pride.\"\"\"\n",
    "ids = tokenizer.encode(text)\n",
    "print(ids)\n",
    "\n",
    "tokenizer.decode(ids)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "3cd76184-275a-4606-aedf-86addbf6bf76",
   "metadata": {},
   "source": [
    "text = \"Hello, do you like tea?\"\n",
    "#print(tokenizer.encode(text))"
   ]
  },
  {
   "cell_type": "raw",
   "id": "110443ab-31cf-4b45-8642-5f82ac81925e",
   "metadata": {},
   "source": [
    "all_tokens = sorted(set(preprocessed))\n",
    "all_tokens.extend([\"<|endoftext|>\", \"<|unk|>\"])\n",
    "vocab = {token:integer for integer,token in enumerate(all_tokens)}\n",
    "len(vocab)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "4591a0ff-3478-4b06-9d75-11b84945c145",
   "metadata": {},
   "source": [
    "class SimpleTokenizerV2:\n",
    "    def __init__(self, vocab):\n",
    "        self.toInt = vocab\n",
    "        self.toStr = {integer:token for token, integer in vocab.items()}\n",
    "\n",
    "    def encode(self, text):\n",
    "        preprocessed = re.split(r'([,.:;?_!()\\']|--|\\s)', text)\n",
    "        preprocessed = [item.strip() for item in preprocessed if item.strip()]\n",
    "        preprocessed = [item if item in self.toInt else \"<|unk|>\" for item in preprocessed]\n",
    "        ids = [self.toInt[s] for s in preprocessed]\n",
    "        return ids\n",
    "\n",
    "    def decode(self, ids):\n",
    "        text = \" \".join([self.toStr[i] for i in ids])\n",
    "        text = re.sub(r'\\s+([,.?!\"()\\'])', r'\\1', text)\n",
    "        return text"
   ]
  },
  {
   "cell_type": "raw",
   "id": "f12167c3-d111-4bed-be9f-2a5fb539d435",
   "metadata": {},
   "source": [
    "tokenizer = SimpleTokenizerV2(vocab)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "85359768-f339-4c1b-8e54-1c10e4b1956a",
   "metadata": {},
   "source": [
    "text1 = \"Hello, do you like tea?\"\n",
    "text2 = \"In the sunlit terraces of the palace.\"\n",
    "text = \" <|endoftext|> \".join((text1, text2))\n",
    "print('text: ', text)\n",
    "ids = tokenizer.encode(text)\n",
    "print(ids)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "04571cef-a74b-499d-b3ff-93e85052b635",
   "metadata": {},
   "source": [
    "tokenizer.decode(ids)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "117be90b-929b-4382-851f-d3626a5dd918",
   "metadata": {},
   "source": [
    "## BYTE PAIR ENCODING\n",
    "Implementing BPE from scratch can be relatively complicated, thus we will use an existing Python open-source library called tiktoken which is a fast BPE tokenizer for use wuth OPENAI's models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "afac4977-0770-4205-a8ec-404097571573",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tiktoken version:  0.9.0\n"
     ]
    }
   ],
   "source": [
    "import tiktoken\n",
    "import importlib\n",
    "print('tiktoken version: ', importlib.metadata.version(\"tiktoken\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "5775dcc9-1ac5-4df1-afac-6884d050fe18",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = tiktoken.get_encoding(\"gpt2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "654b70dd-8292-4951-ae05-bafe36313bf8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[15496, 11, 466, 345, 588, 8887, 30, 220, 50256, 554, 262, 4252, 18250, 8812, 2114, 286, 617, 34680, 27271, 13]\n"
     ]
    }
   ],
   "source": [
    "text = ( \"Hello, do you like tea? <|endoftext|> In the sunlit terraces of someunknownPlace.\" )\n",
    "\n",
    "integers = tokenizer.encode(text, allowed_special={\"<|endoftext|>\"})\n",
    "print(integers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "2d80a41c-654c-4bab-9674-d743653faf38",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello, do you like tea? <|endoftext|> In the sunlit terraces of someunknownPlace.\n"
     ]
    }
   ],
   "source": [
    "strings = tokenizer.decode(integers)\n",
    "print(strings)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b1850a7-5825-4365-9c29-77af4fda8dd2",
   "metadata": {},
   "source": [
    "The BPE tokenizer can handle unknown words. How can it achieve this without using <|unk|> token?\n",
    "\n",
    "\n",
    "The algorithm underlying BPE breaksdown words that aren't in its predefined vocabulary into smaller subword units or even individual characters.\n",
    "This enables it to handle out of vocabulary(OOV) words."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63e82bf2-56ea-4a75-8536-92edfab8027d",
   "metadata": {},
   "source": [
    "An example to illustrate how the BPE tokenizer deals with unknown tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "58497384-078d-481d-b712-96fe37070a1b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "integers:  [33901, 86, 343, 86, 220, 959]\n",
      "strings:  Akwirw ier\n"
     ]
    }
   ],
   "source": [
    "integers = tokenizer.encode(\"Akwirw ier\")\n",
    "print(\"integers: \", integers)\n",
    "\n",
    "strings = tokenizer.decode(integers)\n",
    "print(\"strings: \", strings)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf9d1f54-33d8-4ac4-8603-5656d5a1a04e",
   "metadata": {},
   "source": [
    "## Creating input-output layer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2ef66ff-2143-4cfb-bfe9-4e34f61c5a19",
   "metadata": {},
   "source": [
    "We implement a data loader that fetches the input-output pairs using a sliding window approach."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "5d81f116-5756-45b1-aaf1-21781a91725c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of enc_text: 5145\n"
     ]
    }
   ],
   "source": [
    "enc_text = tokenizer.encode(raw_text)\n",
    "print(f\"Length of enc_text: {len(enc_text)}\")\n",
    "enc_sample = enc_text[50:]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9586493-8563-4dc5-83dd-f20e7c7c1bdc",
   "metadata": {},
   "source": [
    "## Context size \n",
    "Determines how many tokens are included in the input. The model istrained to look at a sequence of context_size number of words to predict the next word in the sequence.\n",
    "\n",
    "Each input-output pair contains context size number of prediction tasks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "6aa5f2d8-99b9-4ab1-ba12-9406a324c852",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x:  [290, 4920, 2241, 287]\n",
      "y:    [4920, 2241, 287, 257]\n"
     ]
    }
   ],
   "source": [
    "context_size = 4\n",
    "x = enc_sample[:context_size]\n",
    "y = enc_sample[1:context_size+1]\n",
    "\n",
    "print(f\"x:  {x}\")\n",
    "print(f\"y:    {y}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "fb71113f-70a8-4d2f-8f5e-1bb3fe73794f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[290] ----> 4920\n",
      "[290, 4920] ----> 2241\n",
      "[290, 4920, 2241] ----> 287\n",
      "[290, 4920, 2241, 287] ----> 257\n"
     ]
    }
   ],
   "source": [
    "for i in range(1, context_size+1):\n",
    "    context = enc_sample[:i]\n",
    "    desired = enc_sample[i]\n",
    "    print(context, \"---->\", desired)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "38d3e8ff-85b8-4633-ad52-83345ab9949f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " and ---->  established\n",
      " and established ---->  himself\n",
      " and established himself ---->  in\n",
      " and established himself in ---->  a\n"
     ]
    }
   ],
   "source": [
    "for i in range(1, context_size+1):\n",
    "    context = enc_sample[:i]\n",
    "    desired = enc_sample[i]\n",
    "    print(tokenizer.decode(context), \"---->\", tokenizer.decode([desired]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d47d118b-5301-4608-8bcd-5fb62d53d0f3",
   "metadata": {},
   "source": [
    "## Data Loader\n",
    "Iterates over the input dataset and returns inputs and targets as PyTorch tensors. We are iterested in returning two tensors: an input tensor containing text that the LLM sees and a target tensor that includes the trget for LLM to predict. We implement dataloader using PyTorch datasets and dataloader classes. We aim at returning two tensors: an input tensor and an output tensor. Helps us do parallel processing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "c7425667-1f25-40ef-9f3d-2ce2faf7d021",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "class GPTDatasetV1(Dataset):\n",
    "    def __init__(self, txt, tokenizer, max_length, stride):\n",
    "        self.input_ids = []\n",
    "        self.target_ids = []\n",
    "        self.token_ids = tokenizer.encode(txt, allowed_special={\"<|endoftext|>\"})\n",
    "\n",
    "        for i in range(0, len(self.token_ids)-max_length, stride):\n",
    "            input_chunk = self.token_ids[i : max_length+i]\n",
    "            target_chunk = self.token_ids[i+1 : i + max_length + 1]\n",
    "            self.input_ids.append(torch.tensor(input_chunk))\n",
    "            self.target_ids.append(torch.tensor(target_chunk))\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.input_ids)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.input_ids[idx], self.target_ids[idx] #idx=index"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0296a620-cdf1-449f-ac73-29e49c71c629",
   "metadata": {},
   "source": [
    "drop_last=true = drops the last batchif it is shorter than the specified batch_size to prevent loss spikes during training.\n",
    "\n",
    "batch_size = how many batches or CPU processes we want to run parallelly\n",
    "\n",
    "max_length = context length\n",
    "\n",
    "num_workers = number of CPU threads which we can run simultaneously. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "212c711e-807d-446a-a6f2-05051cb15ed5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_dataloader_v1(txt, batch_size=4, max_length=256, stride=128, shuffle=True, drop_last=True, num_workers=0):\n",
    "    tokenizer = tiktoken.get_encoding(\"o200k_base\")\n",
    "    dataset = GPTDatasetV1(txt, tokenizer, max_length, stride) #creating dataset\n",
    "    dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=shuffle, drop_last=drop_last, num_workers=num_workers)\n",
    "    return dataloader\n",
    "\n",
    "# this function governs batch processing or the parallel processing we need which is governed by the batch size.\n",
    "#It help us create the input output data pairs from the dataset which we defined earlier.   "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8dcdec02-b511-4df8-a1da-1e0e6eeaa1e9",
   "metadata": {},
   "source": [
    "We now convert the dataloader to python iterator to fetch the next entry via python built-in next() function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be00c565-6230-4476-bc78-f1954761b76a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "print(\"PyTorch Version: \", torch.__version__)\n",
    "dataloader = create_dataloader_v1(raw_text, batch_size=8, max_length=4, stride=4, shuffle=False)\n",
    "data_iter = iter(dataloader)\n",
    "inputs, targets = next(data_iter)\n",
    "print(\"Inputs: \", inputs)\n",
    "print(\"targets: \", targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f71efdd0-9b2b-4a75-b1f8-b7e81dc5b22f",
   "metadata": {},
   "outputs": [],
   "source": [
    "second_batch = next(data_iter)\n",
    "print(second_batch)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f747d6cf-d312-4844-a4a0-e3d392f77570",
   "metadata": {},
   "source": [
    "Batch size of 1 are used for illustration puposes. Small batch sizes require less memort during training but lead to more noisy model updates.\n",
    "\n",
    "Batch size is a trade-off and hyperparameter to experiment with when training LLMs.\n",
    "\n",
    "Model will procces one batch before making the parameter updates."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c8b632d-2d15-4e1f-9574-e5b0b33e7e44",
   "metadata": {},
   "source": [
    "## TOKEN EMBEDDINGS"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f9f760d-a5bf-4bb1-8ead-f16a0869ceb1",
   "metadata": {},
   "source": [
    "For demonstration purpose we create an embeding later for a vocab of cardinality 6 projected to R3."
   ]
  },
  {
   "cell_type": "raw",
   "id": "2bee72ab-52a9-4798-9277-a787ea865040",
   "metadata": {},
   "source": [
    "vocab_size = 6\n",
    "output_dim = 3\n",
    "\n",
    "torch.manual_seed(123)\n",
    "embedding_layer = torch.nn.Embedding(vocab_size, output_dim)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "7045b650-a1ee-4b29-98e4-84a9fa10e11c",
   "metadata": {},
   "source": [
    "print(\"weights:\\n\", embedding_layer.weight)\n",
    "print(\"third vector:\\n\", embedding_layer(torch.tensor([3])))"
   ]
  },
  {
   "cell_type": "raw",
   "id": "9c6f0aad-8e63-42df-91cd-b27001f8925f",
   "metadata": {},
   "source": [
    "input_ids = torch.tensor([2,3,5,1])"
   ]
  },
  {
   "cell_type": "raw",
   "id": "96cc0031-a62c-43d3-b2b7-d4035132cce5",
   "metadata": {},
   "source": [
    "print(embedding_layer(input_ids))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c920a57-1d8c-4962-9f4a-434740afd9fd",
   "metadata": {},
   "source": [
    "## Positional encoding/embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57affa97-ab0c-489e-b842-3041ff29b89a",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size = 50257\n",
    "output_dim = 256\n",
    "\n",
    "token_embedding_layer = torch.nn.Embedding(vocab_size, output_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c077286-8990-4147-b786-ac68660cbafc",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_length = 4\n",
    "dataloader = create_dataloader_v1(raw_text, batch_size=8, max_length=max_length, stride=max_length, shuffle=False)\n",
    "\n",
    "data_iter = iter(dataloader)\n",
    "inputs, targets = next(data_iter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca9e6e5b-6f19-4181-8cc7-e5ac006e4aee",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Token IDs:\\n\", inputs)\n",
    "print(\"\\nInputs shape:\\n\", inputs.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "411cef3b-edbb-41f0-9346-24181a170aff",
   "metadata": {},
   "source": [
    "Now we convert each token id to a 256 dimensional vector."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1177d182-eaf6-43af-8fb0-f8d7cb3386db",
   "metadata": {},
   "outputs": [],
   "source": [
    "token_embeddings = token_embedding_layer(inputs)\n",
    "print(token_embeddings.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17632911-0818-4797-a475-2dc314a44dfd",
   "metadata": {},
   "source": [
    "Embedding layer for positional embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d0bd2e7-27be-4d0f-b02b-6091520a93e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "context_length = max_length\n",
    "pos_embedding_layer = torch.nn.Embedding(context_length, output_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9198a1c4-3b04-478d-95e0-2772c38b8a91",
   "metadata": {},
   "outputs": [],
   "source": [
    "pos_embeddings = pos_embedding_layer(torch.arange(max_length))\n",
    "print(pos_embeddings.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b93848d-4047-4bb4-ba81-f42d44f162f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "token_embedding = input_embeddings + positonal_embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0ba10a32-cda2-44ff-b774-702651b1a515",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bb943f43-f327-4aa7-a772-32a0b2bf59e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = torch.tensor([[0.43, 0.15, 0.89], #your\n",
    "                       [0.55, 0.87, 0.66], #journey\n",
    "                       [0.57, 0.85, 0.64], #starts\n",
    "                       [0.22, 0.58, 0.33], #with\n",
    "                       [0.77, 0.25, 0.10], #one\n",
    "                       [0.05, 0.80, 0.55]]) #step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5acf58cf-3305-4813-b340-18a7240d73a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.mplot3d import Axes3D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3c058236-e7b6-4d96-91ce-a294ca6d7b85",
   "metadata": {},
   "outputs": [],
   "source": [
    "words = [\"Your\", \"journey\", \"srats\", \"with\", \"one\", \"step\"]\n",
    "\n",
    "x_coords = inputs[:, 0].numpy()\n",
    "y_coords = inputs[:, 1].numpy()\n",
    "z_coords = inputs[:, 2].numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4028180-cb3f-4408-bbec-97518738cef8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ebe2d82-2c4c-48ff-a933-0fd06ae2470f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4706d326-bb58-442b-9ce4-ca01232b1b25",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a942e31c-be39-4a52-9d94-961c4486989d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a22a07d5-35c9-4889-b9ab-88886237bd3b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0843c59b-73da-478a-98e7-1560050fb91c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0b08236-6be3-4b42-896c-e979b2d35270",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b74015b-3e14-4243-b911-e01a0adc9dfc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3add55ce-0c50-42e8-857b-2ed858b974d7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9921dfa-ca1a-4796-8a8b-8f34f5218f69",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ba42dba-2113-4f7a-be30-343988401fe2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ebf8a8d-9f0f-447b-b0f6-86eeeba8ea54",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e884e515-263d-4652-85e5-65c9ad13d538",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a90b43a3-1a72-4c17-a348-3be3baf0aa78",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5025c568-8ed4-425b-b443-50ff7a3a4437",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3888856-51b6-4b33-8dfe-e7caee6d6d80",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbc0383a-99ff-4bf6-b044-a62284ea7169",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49b25336-6e1a-43d1-9683-ff91bd5186c2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d498bd0-3d41-439e-9f63-c32ed9a50dfa",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "861b50bb-27ad-40de-a1d3-754d0a05280c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2e621eb-4dd6-4546-b3a0-d0ac2d1d7a1d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a7d4064-4903-416f-9fb2-3e1e6f3a8a86",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8db529cb-89bf-4809-a0ab-5f2d15e5cfd5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cfeaf5f-3564-470d-b150-ac5ec9086825",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a8d1559-9704-4bb4-9a1d-ca5d2cc83962",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e23d7ca-682d-4e05-8917-b3b822c31cca",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb2046d2-dbdf-4438-82b8-f88b0c3f9828",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "264b12d8-b39b-4965-adf2-4aa908ab6e5c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86c8213c-aba9-4fee-b7fa-8a9ee219f886",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e5e50b0-e7b5-4359-a52d-c5ba8fee9159",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99798b78-e005-4880-be57-6fe8a4960863",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da8460e7-69e7-49af-b840-2183e1c47ff8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc3196d1-490d-4c10-9484-4f6cd450505a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "532ecc11-4e50-4bc5-91d8-0787f3535e1b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b2303fe-c435-477c-86b0-d262f5cd606c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd24f87b-3243-4f0b-8117-01a414bf8c18",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e508fda-e70a-4d3a-b20b-7a9d7c90d4b2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abb650db-1e0e-4eb1-9b7f-6064157c741e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa2d33ee-1990-40d8-bc4a-ae4d999bfb6b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ad96a27-1190-4e77-bced-afb6c45b5828",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e4ae173-7953-4524-bc18-0bb57741256e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25ff947c-9a81-4f5e-95dd-bff795794960",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fa3b9a6-9ec2-46c1-a0b8-c6e9afc1279a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5d3efb4-8e1b-4f64-96e5-97f3d94922b7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2922834a-8d71-4e89-a7e4-3c125d54e954",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1354dcb-abe7-4f92-8f3c-daa8bb047156",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
