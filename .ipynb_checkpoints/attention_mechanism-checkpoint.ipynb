{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "1e612672-9d8f-4e5e-bf52-8364ac384674",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "4fc5878d-cb27-42a1-b6ee-5d4e5525c2e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = [[0.43, 0.15, 0.89],\n",
    "        [0.55, 0.87, 0.66],\n",
    "        [0.57, 0.85, 0.64],\n",
    "        [0.22, 0.58, 0.33],\n",
    "        [0.77, 0.25, 0.10],\n",
    "        [0.05, 0.80, 0.55]]\n",
    "inputs = torch.tensor(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3e2cbf5-f14e-46c2-a52f-d91fe800c216",
   "metadata": {},
   "source": [
    "## A simple self-attention mechanishm without trainable weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "a4054beb-f5b5-47e3-9713-0a8d379e2aef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Context vector for all input tokens:\n",
      " tensor([[0.4421, 0.5931, 0.5790],\n",
      "        [0.4419, 0.6515, 0.5683],\n",
      "        [0.4431, 0.6496, 0.5671],\n",
      "        [0.4304, 0.6298, 0.5510],\n",
      "        [0.4671, 0.5910, 0.5266],\n",
      "        [0.4177, 0.6503, 0.5645]])\n"
     ]
    }
   ],
   "source": [
    "attn_scores = torch.zeros(6,6)\n",
    "attn_weights = torch.zeros(6,6)\n",
    "\n",
    "attn_scores = inputs @ inputs.T\n",
    "attn_weights = torch.softmax(attn_scores, dim=-1)\n",
    "context_vectors = attn_weights @ inputs\n",
    "\n",
    "print(f\"Context vector for all input tokens:\\n {context_vectors}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c80f1023-cc7d-43bb-8db5-645304c6099e",
   "metadata": {},
   "source": [
    "## Implementing self-attention weights with trainable weights\n",
    "\n",
    "Initializing three weights: W_query, W_key, W_value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "4cea9070-8c3d-42f2-8bb2-97f705d5e8ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "d_in = inputs.shape[1]\n",
    "d_out = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "61e17606-5879-4ba3-a22e-d0f817926fff",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SelfAttention_v1(nn.Module):\n",
    "    def __init__(self, d_in, d_out):\n",
    "        super().__init__()\n",
    "        self.w_key = nn.Parameter(torch.rand(d_in, d_out))\n",
    "        self.w_query = nn.Parameter(torch.rand(d_in, d_out))\n",
    "        self.w_value = nn.Parameter(torch.rand(d_in, d_out))\n",
    "    def forward(self, x):\n",
    "        keys = x @ self.w_key\n",
    "        queries = x @ self.w_query\n",
    "        values = x @ self.w_value\n",
    "        attention_scores = queries @ keys.T\n",
    "        attenton_weights = torch.softmax(attention_scores / keys.shape[-1]**0.5, dim=-1)\n",
    "        context_vector = attenton_weights @ values\n",
    "        return context_vector\n",
    "\n",
    "class SelfAttention_v2(nn.Module):\n",
    "    def __init__(self, d_in, d_out, qkv_bias=False):\n",
    "        super().__init__()\n",
    "        self.w_key = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
    "        self.w_query = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
    "        self.w_value = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
    "    def forward(self, x):\n",
    "        keys = self.w_key(x)\n",
    "        queries = self.w_query(x)\n",
    "        values = self.w_value(x)\n",
    "        attention_scores = queries @ keys.T\n",
    "        attention_weights = torch.softmax(attention_scores / keys.shape[-1]**0.5, dim=-1)\n",
    "        context_vector = attention_weights @ values\n",
    "        return attention_scores, context_vector    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "df3fb809-4644-4c0c-b994-f8edc8b095da",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Forward pass on inputs in version 1:\n",
      " tensor([[0.2947, 0.7956],\n",
      "        [0.3015, 0.8132],\n",
      "        [0.3010, 0.8120],\n",
      "        [0.2925, 0.7902],\n",
      "        [0.2863, 0.7737],\n",
      "        [0.2979, 0.8043]], grad_fn=<MmBackward0>)\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(123)\n",
    "sa_v1 = SelfAttention_v1(d_in, d_out)\n",
    "print(f\"Forward pass on inputs in version 1:\\n {sa_v1.forward(inputs)}\")\n",
    "\n",
    "torch.manual_seed(789)\n",
    "sa_v2 = SelfAttention_v2(d_in, d_out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "353e3750-70b9-4fb6-a2bb-88e5c214a623",
   "metadata": {},
   "outputs": [],
   "source": [
    "attention_scores, context_vector = sa_v2.forward(inputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8163dbcf-a72d-4e4b-9673-fe9f27864f5d",
   "metadata": {},
   "source": [
    "## CAUSAL ATTENTION / MASKED ATTENTION\n",
    "\n",
    "Modified attention mechanism to prevent the model from accessng future information in the sequence, which is crucial for tasks like\n",
    "language modeling, where each word prediction should depend on previous word.\n",
    "\n",
    "It restricts the model to only consider previous and current inputs in a sequence when processing any given token while computing computing attention score and hene context vector.\n",
    "\n",
    "In contrast, where the self attention mechanism allows access to the entire input sequence at once."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "4aae0905-873f-4ea4-a398-34746091e746",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Attention weights:\n",
      " tensor([[1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.4822, 0.5178, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.3181, 0.3412, 0.3407, 0.0000, 0.0000, 0.0000],\n",
      "        [0.2451, 0.2521, 0.2521, 0.2508, 0.0000, 0.0000],\n",
      "        [0.2002, 0.2063, 0.2060, 0.1950, 0.1925, 0.0000],\n",
      "        [0.1607, 0.1676, 0.1677, 0.1681, 0.1683, 0.1675]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n"
     ]
    }
   ],
   "source": [
    "context_length = 6\n",
    "keys = nn.Parameter(torch.rand(d_in, d_out))\n",
    "\n",
    "mask = torch.triu(torch.ones(context_length, context_length), diagonal=1)\n",
    "masked = attention_scores.masked_fill(mask.bool(), -torch.inf)\n",
    "\n",
    "attention_weights = torch.softmax(masked / keys.shape[-1]**0.5, dim=-1)\n",
    "print(f\"Attention weights:\\n {attention_weights}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "321be439-259e-4db9-80af-77ebc37d1401",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch = torch.stack((inputs, inputs), dim=0) #input text is duplicated to simulate batch inputs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "420865df-12a5-46f1-884e-9cb5ff019368",
   "metadata": {},
   "source": [
    "Register buffer are automatically moved to the appropriate device(CPU or GPU) along with out model, which will be relevant when training LLM. That is, we don't need to manually ensure these tensors are on the same device as your model parameters, avoiding device mismatch errors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "d6f6939f-1ea5-4421-bb4f-b7ac7325924d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CausalAttention(nn.Module):\n",
    "    def __init__(self, d_in, d_out, context_length, dropout, qkv_bias=False):\n",
    "        super().__init__()\n",
    "        self.d_out = d_out\n",
    "        self.w_key = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
    "        self.w_query = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
    "        self.w_value = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.register_buffer('mask', torch.triu(torch.ones(context_length, context_length), diagonal=1))\n",
    "\n",
    "    def forward(self, x):\n",
    "        num_batch, num_tokens, d_in = x.shape\n",
    "        keys = self.w_key(x)\n",
    "        queries = self.w_query(x)\n",
    "        values = self.w_value(x)\n",
    "        attention_scores = queries@keys.transpose(1,2) #transposing dimensions 1 and 2 keeping the batch dimensions at first position(0)\n",
    "        attention_scores.masked_fill_(self.mask.bool()[:num_tokens, :num_tokens], -torch.inf)\n",
    "        attention_weights = torch.softmax(attention_scores / keys.shape[-1]**0.5, dim=-1)\n",
    "        attention_weights = self.dropout(attention_weights)\n",
    "        context_vector = attention_weights@values\n",
    "        return attention_scores, context_vector     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "cb9f7bd4-6e39-4389-b623-d34c6f2e9335",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(123)\n",
    "context_length = batch.shape[1]\n",
    "ca = CausalAttention(d_in, d_out, context_length, 0.0)\n",
    "attention_scores, context_vectors = ca(batch)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9cc7b97c-3d6d-4236-ac40-ce2210d1f3fd",
   "metadata": {},
   "source": [
    "## MULTI-HEAD ATTENTION\n",
    "\n",
    "Splitting attention mechanism into multiple heads.\n",
    "Each head learns different aspects of the data, allowing model to simultaneously attend to information from different representation subspaces at different positions, hence improving model's performance in complex tasks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "c1e4d2d4-ca53-432e-bb16-f0f8b1fe8fd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttentionWraper(nn.Module):\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "4e9771e4-51ec-4cb8-8baa-28b28152cd94",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention(nn.Module):\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99fdcdc9-62a9-41ab-aabc-7203885e9785",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2dcab1c-6ffa-4c67-8bf2-632cf9f4800e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1743c21a-e8c4-4b07-99d0-bd151e81e646",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2632d21c-d8d0-4eca-8e00-a8b4436180e5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ed83242-25c5-486a-8bde-34d2f4806dfb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2fec634-a3d9-46d6-84a7-27da57d9d41e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0113d7f2-6765-42a8-93c3-956af9227e3e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a92b71b1-28c7-403d-aab5-00b6329568b8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c767a35c-8d61-4a67-aaa0-320d892fc053",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c454818-6f60-4fa9-ac10-b18a8d6b47c3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78b6fdc3-2cdf-4663-8377-c5e0fcd15705",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "778aa97b-e449-4105-94af-48381a6223c5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7717b87-b26e-46b0-9a85-a64fcca7bb18",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e801adae-6188-44bf-a408-779b989049a5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "076f1064-45ef-43cd-99a9-63e347c9bfc3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3fed4e6-9176-4f30-a141-c58c57885afa",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5017e59-abca-4613-9868-6679967fde52",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7470f94b-8ed1-4234-89d8-86b3e5968ede",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9ef40e8-762d-4edc-b729-bb729ab28744",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89a8901e-f40c-41ec-9a30-ac3372326f21",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1797eb96-ba8a-4e46-ba6d-d65ce74f4ebe",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c90ecd6f-c3a2-4970-821c-a71d0b1ab3a3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "712dbf58-1091-42f3-9038-0635aaec815d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80e59caa-6ba9-4ebe-80de-e044e9565b0f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4dc0baec-9fe7-4c6b-b25b-e6a5b551d4c7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87347b28-170e-464e-8d69-aecf4d625c3a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
