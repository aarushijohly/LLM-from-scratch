{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bf406af8-725b-413d-a2e8-a6183e1d5a9c",
   "metadata": {},
   "source": [
    "Creating input-target pairs. We implement a data loader that fetches the input-target pairs using a sliding window approach. We start with tokenizing the whole the Verdict story using the BPE tokenizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "208e7df3-fc2a-48e8-a75e-c51ba56b1f2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tiktoken\n",
    "tokenizer = tiktoken.get_encoding(\"gpt2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5245e927-1467-49d6-b22a-d998ee1fcbf6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "length enc_text:  5145\n",
      "I HAD always thought Jack Gisburn rather a cheap genius--tho\n",
      "[40, 367, 2885, 1464, 1807, 3619, 402, 271, 10899, 2138, 257, 7026, 15632, 438, 2016, 257, 922, 5891, 1576, 438, 568, 340, 373, 645, 1049, 5975, 284, 502, 284, 3285, 326, 11, 287, 262, 6001, 286, 465, 13476, 11, 339, 550, 5710, 465, 12036, 11, 6405, 257, 5527, 27075, 11, 290, 4920, 2241, 287, 257, 4489, 64, 319, 262, 34686]\n"
     ]
    }
   ],
   "source": [
    "with open(\"the-verdict.txt\", \"r\", encoding=\"utf-8\") as f:\n",
    "    raw_text = f.read()\n",
    "\n",
    "enc_text = tokenizer.encode(raw_text)\n",
    "print('length enc_text: ', len(enc_text))\n",
    "print(raw_text[:60])\n",
    "print(enc_text[:60])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "729ea5a7-15df-418c-9c2c-4b5489f1acc3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[290, 4920, 2241, 287]\n",
      "x: [290, 4920, 2241, 287]\n",
      "y:      [4920, 2241, 287, 257]\n"
     ]
    }
   ],
   "source": [
    "enc_sample = enc_text[50:]\n",
    "print(enc_sample[:4])\n",
    "#we remove first 50 tokens from the dataset for demonstration purpose as it results in a slightly more intresting text passage\n",
    "\n",
    "\n",
    "context_size = 4\n",
    "# context size 4 means that the model is trained to look at a sequence of 4 words to predict the next word in the sequnce.\n",
    "\n",
    "x = enc_sample[:context_size]\n",
    "y = enc_sample[1:context_size+1]\n",
    "\n",
    "print(f\"x: {x}\")\n",
    "print(f\"y:      {y}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32003c63-01e4-4556-80b7-1eb74d7152cf",
   "metadata": {},
   "source": [
    "Processing the inputs along with the targets, which is the inputs shifted by one position, we can create the next-word prediction tasks as follows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c589ec4b-b650-4975-b9b2-77c66e23f274",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[290] ---> 4920\n",
      "[290, 4920] ---> 2241\n",
      "[290, 4920, 2241] ---> 287\n",
      "[290, 4920, 2241, 287] ---> 257\n"
     ]
    }
   ],
   "source": [
    "for i in range(1, context_size+1):\n",
    "    context = enc_sample[:i]\n",
    "    desired = enc_sample[i]\n",
    "    print(context, \"--->\", desired)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b6201cf-d92e-46d2-a0d8-77930165b601",
   "metadata": {},
   "source": [
    "Everything on the left of the arrow refers to the input an LLM would receive, and the token ID on the right side of the arrow represents the target token ID that the LLM is supposed to predict."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f5e4093-23d7-47b2-9ce2-60deac05ec16",
   "metadata": {},
   "source": [
    "We now code to convert the token IDS into text:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1f8aa6f0-4235-4130-9d9f-fed0b2c84e1a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " and --->  established\n",
      " and established --->  himself\n",
      " and established himself --->  in\n",
      " and established himself in --->  a\n"
     ]
    }
   ],
   "source": [
    "for i in range(1, context_size+1):\n",
    "    context = enc_sample[:i]\n",
    "    desired = enc_sample[i]\n",
    "    print(tokenizer.decode(context), \"--->\", tokenizer.decode([desired]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f228a70-217c-4969-8335-6df6cd29bd8d",
   "metadata": {},
   "source": [
    "## IMPLEMENTING A DATA LOADER\n",
    "using PyTorch's built-in Datasets and DataLoader classes\n",
    "\n",
    "Step 1: Tokenize the entire text\n",
    "\n",
    "Step 2: Use a sliding window to chunk the book into overlapping sequence of max_length.\n",
    "\n",
    "Step 3: return the total number of rows in the dataset\n",
    "\n",
    "Step 4: Return  single row from the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "1dea9a64-44bc-407a-afb7-6b013278120d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset, DataLoader\n",
    "#dataset needs to be in input output pairs\n",
    "class GPTDatasetV1(Dataset):\n",
    "    def __init__(self, txt, tokenizer, max_length, stride): #max length = context size\n",
    "        self.input_ids = []\n",
    "        self.target_ids = []\n",
    "        #tokenize the entire text\n",
    "        token_ids = tokenizer.encode(txt, allowed_special={\"<|endoftxt|>\"})\n",
    "        #using a sliding window to chunk the book into overlapping sequences of max_length\n",
    "        for i in range(0, len(token_ids)-max_length, stride):\n",
    "            input_chunk = token_ids[i:i+max_length]\n",
    "            target_chunk = token_ids[i+1: i+max_length+1]\n",
    "            self.input_ids.append(torch.tensor(input_chunk))\n",
    "            self.target_ids.append(torch.tensor(target_chunk))\n",
    "            \n",
    "    def __len__(self):\n",
    "        return len(self.input_ids)\n",
    "        \n",
    "    #method which will be used by the data loader\n",
    "    def __getitem__(self, idx): #idx=index\n",
    "        return self.input_ids[idx], self.target_ids[idx] #based on the index provided it will return that particular row of input and output\n",
    "\n",
    "# data loader needs dataset in map style or iterable style, here we are using map style"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69a9a368-45ea-485e-a3be-f1963c171cf4",
   "metadata": {},
   "source": [
    "The following code will use the GPTDatasetV1 to load the inputs in batches via Pytorch Dataloader:\n",
    "\n",
    "Step 1: Initialize the tokenizer\n",
    "\n",
    "Step 2: Create dataset\n",
    "\n",
    "Step 3: drop_last = True drops the last batch if it is shorter than the specified batch size to prevent loss spikes during training\n",
    "\n",
    "Step 4: The number of CPU processes to use for preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "dae44f49-a40b-482f-adc3-40555d9f7c4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#This function will implement the batch processing, parallel processing which will be required, governed by the batch size.\n",
    "#This function help us create the input output data pairs from the dataset which we defined earlier.\n",
    "#num_workers=number of cpu threads which we can run simultaneously\n",
    "#batch size=how many cpu processes you want to run parallel\n",
    "#max_length=context length\n",
    "\n",
    "def create_dataloader_v1(txt, batch_size=4, max_length=256, stride=128, shuffle=True, drop_last=True, num_workers=0):\n",
    "    #initialize the tokenizer\n",
    "    tokenizer = tiktoken.get_encoding(\"gpt2\")\n",
    "    #create dataset\n",
    "    dataset = GPTDatasetV1(txt, tokenizer, max_length, stride)\n",
    "    #create dataloader, this function will check the get item method in above function and it will return the input output pairs based on what\n",
    "    #is mentioned in the get item.\n",
    "    dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=shuffle, drop_last=drop_last, num_workers=num_workers)\n",
    "    return dataloader"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39947ca1-b10f-44b3-99f8-ebfb39051979",
   "metadata": {},
   "source": [
    "There is a difference between batch size and number of workers.\n",
    "\n",
    "Batch size=number of batches the model processed at once before updating its parameters. To make sure that the model updates its parameters quickly data is usually chunked into batches, so that after analysing 4 batches the model will update its parameters, rather than going to the entire dataset.\n",
    "\n",
    "Num_workers is for parallel processing on different threads of the cpu.\n",
    "\n",
    "create_dataloader_V1 helps us do all this, alse defining batch size, num workers would be very challenging."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e81d9ed5-1cfa-4005-b6ef-847244d0e69e",
   "metadata": {},
   "source": [
    "Testing the dataloader with a batch size=1 for an LLM with a context size of 4. Which helps develop the intuition of how create_dataloader_V1 and GPTDatasetV1 works together."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "b84c725b-7a15-42c8-9230-ebd47e426fc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"the-verdict.txt\", 'r', encoding=\"utf-8\") as f:\n",
    "    raw_text = f.read()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df767ff4-67d2-4915-8370-063fbb94beec",
   "metadata": {},
   "source": [
    "Now we create a data loader and convert the dataloader into a python iterator to fetch the next entry in the dataset via Python's build-in next() function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "e014605d-2ce7-4812-9e43-2076ed46ec59",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyTorch version: 2.5.1+cpu\n",
      "[tensor([[  40,  367, 2885, 1464]]), tensor([[ 367, 2885, 1464, 1807]])]\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "print(\"PyTorch version:\", torch.__version__)\n",
    "dataloader = create_dataloader_v1(raw_text, batch_size=1, max_length=4, stride=1, shuffle=False)\n",
    "\n",
    "data_iter = iter(dataloader)\n",
    "first_batch = next(data_iter)\n",
    "print(first_batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "5ef5b4e1-80c0-4227-b203-51466d401099",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[tensor([[ 367, 2885, 1464, 1807]]), tensor([[2885, 1464, 1807, 3619]])]"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "second_batch=next(data_iter)\n",
    "second_batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d441d462-a988-4cb8-892e-143691c3760b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba53613f-cf8c-444e-8231-cc4c361ec15a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ba52480-62fd-45bd-91d9-6d7e67436c27",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
